{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:03:46.654634Z",
     "start_time": "2020-03-28T23:03:46.651254Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:03:47.642730Z",
     "start_time": "2020-03-28T23:03:47.637519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# findspark.init('/home/brunowdev/spark-2.4.5-bin-hadoop2.6/')\n",
    "\n",
    "findspark.init('/home/bruno/LIBS/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:04:06.032570Z",
     "start_time": "2020-03-28T23:04:05.611254Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:04:19.007443Z",
     "start_time": "2020-03-28T23:04:15.811374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import min as smin, max as smax, sum as ssum, round as sround\n",
    "from pyspark.sql.functions import isnan, isnull, when, first, avg, last, count, countDistinct, col, lag, lead, coalesce, lit, split, trim\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_date, date_format, from_unixtime, to_timestamp\n",
    "\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    " \n",
    "import jupyter_utils as j\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, Evaluator\n",
    "from pyspark import since, keyword_only\n",
    "\n",
    " \n",
    "SparkContext.setSystemProperty('spark.logConf', 'True')\n",
    "SparkContext.setSystemProperty('spark.default.parallelism', '16')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '4g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '8g')\n",
    "SparkContext.setSystemProperty('spark.reducer.maxSizeInFlight', '96m')\n",
    "SparkContext.setSystemProperty('spark.shuffle.consolidateFiles', 'True') \n",
    "SparkContext.setSystemProperty('spark.shuffle.service.index.cache.size', '500m')\n",
    "\n",
    "SparkContext.setSystemProperty('spark.driver.extraJavaOptions', '-server -Xmx8G')\n",
    "# SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-server -Xmx8G -XX:+UseG1GC')\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-server -XX:+UseG1GC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T23:39:24.894209Z",
     "start_time": "2020-03-25T23:39:24.887505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j.reload(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:18:42.569285Z",
     "start_time": "2020-03-28T23:18:42.566255Z"
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'sparkify_full_csv_data.csv'\n",
    "filepath = 'medium_sparkify_event_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:18:46.840005Z",
     "start_time": "2020-03-28T23:18:44.918953Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:18:48.125851Z",
     "start_time": "2020-03-28T23:18:48.086773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.extraJavaOptions', '-server -XX:+UseG1GC'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.0.102'),\n",
       " ('spark.shuffle.service.index.cache.size', '500m'),\n",
       " ('spark.reducer.maxSizeInFlight', '96m'),\n",
       " ('spark.shuffle.consolidateFiles', 'True'),\n",
       " ('spark.driver.extraJavaOptions', '-server -Xmx8G'),\n",
       " ('spark.default.parallelism', '16'),\n",
       " ('spark.logConf', 'True'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1585437525741'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.app.name', 'Sparkify'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '35425'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:18:55.499009Z",
     "start_time": "2020-03-28T23:18:51.696475Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").csv(filepath)\n",
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:18:57.846357Z",
     "start_time": "2020-03-28T23:18:57.834988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger instance created\n"
     ]
    }
   ],
   "source": [
    "log4jLogger = spark.sparkContext._jvm.org.apache.log4j\n",
    "\n",
    "LOGGER = log4jLogger.LogManager.getLogger('driver_logger')\n",
    "\n",
    "def info(message, print_on_notebook = True):\n",
    "    LOGGER.info(message)\n",
    "    \n",
    "    if print_on_notebook:\n",
    "        print(message)\n",
    "    \n",
    "info('Logger instance created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:19:01.179125Z",
     "start_time": "2020-03-28T23:19:01.174251Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "def set_storage_on_memory():\n",
    "    info(df.storageLevel)\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)\n",
    "    info(df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:19:03.689063Z",
     "start_time": "2020-03-28T23:19:03.662206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:19:41.766564Z",
     "start_time": "2020-03-28T23:19:41.750280Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_CANCELLATION_PAGE = 'Cancellation Confirmation'\n",
    "REGISTRATION_PAGE = 'Submit Registration'\n",
    "milliseconds_to_hours = 3600 * 1000\n",
    "minutes_to_hours = 60 * 60\n",
    "TRUE = 1\n",
    "FALSE = 0\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \n",
    "    info('Starting data cleaning...')\n",
    "    \n",
    "    total_before = df.count()\n",
    "    \n",
    "    # Keep only logged records\n",
    "    df = df.where(df.auth.isin(['Logged In', 'Cancelled']))\n",
    "    \n",
    "    # Records without userId\n",
    "    df = df.where(col('userId').isNotNull())\n",
    "    \n",
    "    # Create a date column for the event\n",
    "    df = df.withColumn('date', from_unixtime(col('ts') / 1000).cast(DateType()))\n",
    "    \n",
    "    # Location\n",
    "    # df = df.withColumn('state', trim(split((split('location', ',').getItem(1)), '-').getItem(0)))\n",
    "    \n",
    "    # Relevant windows\n",
    "    w_session = Window.partitionBy('sessionId').orderBy('ts')\n",
    "    w_user_session = Window.partitionBy('sessionId', 'userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    w_user = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # Create features\n",
    "    df = df.withColumn('previous_page', lag(df.page).over(w_session))\n",
    "    df = df.withColumn('last_event_ts', last(col('ts')).over(w_user))\n",
    "    df = df.withColumn('last_page', last(col('page')).over(w_user))\n",
    "    df = df.withColumn('register_page', first(col('previous_page')).over(w_user))\n",
    "    df = df.withColumn('first_ts', first(col('ts')).over(w_user))\n",
    "    df = df.withColumn('ts_elapsed', last(df.ts).over(w_session) - first(df.ts).over(w_user_session))\n",
    "    df = df.withColumn('session_duration', smax(df.ts_elapsed).over(w_user_session))\n",
    "     \n",
    "    info('Finished data cleaning...')\n",
    "    info(f'Number of removed rows: {total_before - df.count()}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:19:53.612714Z",
     "start_time": "2020-03-28T23:19:51.909145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Finished data cleaning...\n",
      "Number of removed rows: 15700\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:20:24.958476Z",
     "start_time": "2020-03-28T23:20:24.036158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                page| count|\n",
      "+--------------------+------+\n",
      "|            NextSong|432877|\n",
      "|           Thumbs Up| 23826|\n",
      "|                Home| 19089|\n",
      "|     Add to Playlist| 12349|\n",
      "|          Add Friend|  8087|\n",
      "|         Roll Advert|  7773|\n",
      "|              Logout|  5990|\n",
      "|         Thumbs Down|  4911|\n",
      "|           Downgrade|  3811|\n",
      "|            Settings|  2964|\n",
      "|                Help|  2644|\n",
      "|               About|  1026|\n",
      "|             Upgrade|   968|\n",
      "|       Save Settings|   585|\n",
      "|               Error|   503|\n",
      "|      Submit Upgrade|   287|\n",
      "|    Submit Downgrade|   117|\n",
      "|Cancellation Conf...|    99|\n",
      "|              Cancel|    99|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions about the data:\n",
    "\n",
    "- Are errors related to downgrading canceling the service?\n",
    "- Having a certain number of friends or a sense of community can decrease the churn?\n",
    "- Thumbs down are related to churn? (could the quality of the songs catalog affect the churn)\n",
    "- The advertising is not annoying the users?\n",
    "- Users with stay connected for more time have less change to churn?\n",
    "- Is the home page relevant?\n",
    "- Users, who access the downgrade page are how much more willing to churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:06.259628Z",
     "start_time": "2020-03-08T18:30:05.841058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('status').count().orderBy('count', ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T22:44:01.657485Z",
     "start_time": "2020-03-01T22:44:01.094695Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:16.616860Z",
     "start_time": "2020-03-08T18:30:16.073824Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:29.650593Z",
     "start_time": "2020-03-08T18:30:29.435808Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('userAgent').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:37.323100Z",
     "start_time": "2020-03-08T18:30:36.828209Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92 and song != \\'null\\' ').groupBy('song').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:20:48.715082Z",
     "start_time": "2020-03-28T23:20:48.679146Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_session_dimension(df):\n",
    "    \n",
    "    # sessions from the user\n",
    "    df_sessions = df.orderBy(df.sessionId).groupBy('sessionId', 'userId').agg(\n",
    "        smax(df.ts).alias('max_event_ts'),\n",
    "        smin(df.ts).alias('min_event_ts'),\n",
    "        ssum(df.length).alias('session_n_total_playback'), # Based on songs length\n",
    "        count(when(df.page == 'Thumbs Up', True)).alias(\"session_n_likes\"),\n",
    "        count(when(df.page == 'Thumbs Down', True)).alias(\"session_n_dislikes\"),\n",
    "        count(when(df.page == 'NextSong', True)).alias(\"session_n_songs\"),\n",
    "        count(when(df.page == 'Add Friend', True)).alias(\"session_n_friends\"),\n",
    "        count(when(df.page == 'Add to Playlist', True)).alias(\"session_n_add_playlist\"),\n",
    "        count(when(df.page == 'Home', True)).alias(\"session_n_home\"),\n",
    "        count(when(df.page == 'Roll Advert', True)).alias(\"session_n_ads\"),\n",
    "        count(when(df.page == 'Help', True)).alias(\"session_n_help\"),\n",
    "        count(when(df.page == 'Error', True)).alias(\"session_n_error\"),\n",
    "        count(when(df.page == 'Settings', True)).alias(\"session_n_sets\"),\n",
    "        count(col('page')).alias('session_n_actions'),\n",
    "        first(col('session_duration')).alias('session_duration')\n",
    "    ) \n",
    "    \n",
    "    # Calculate the interval until the next session\n",
    "    w_user_sessions_interval = Window.partitionBy('userId').orderBy('min_event_ts')\n",
    "    df_sessions = df_sessions.withColumn('interval_to_session', col('min_event_ts') - lag(col('max_event_ts')).over(w_user_sessions_interval))\n",
    "    \n",
    "    # Calculate average time in hours for each session\n",
    "    df_session_time = df_sessions.groupBy('userId').agg(\n",
    "       (avg(df_sessions.session_duration) / milliseconds_to_hours).alias('session_hours')\n",
    "    )\n",
    "    df_sessions = df_sessions.join(df_session_time, on = 'userId')\n",
    "    \n",
    "    # We should remove the null lines before count/group to not account 2 times the mean interval\n",
    "    df_sessions = df_sessions.groupBy('userId').agg(  \n",
    "        (avg(df_sessions.interval_to_session) / milliseconds_to_hours).alias('session_avg_time_away'),\n",
    "        ((avg(df_sessions.session_n_total_playback) / minutes_to_hours) / first(col('session_hours'))).alias('session_avg_playback'), \n",
    "        (avg(df_sessions.session_n_likes) / first(col('session_hours'))).alias('session_avg_likes'),\n",
    "        (avg(df_sessions.session_n_dislikes) / first(col('session_hours'))).alias('session_avg_dislikes'),\n",
    "        (avg(df_sessions.session_n_songs) / first(col('session_hours'))).alias('session_avg_songs'),\n",
    "        (avg(df_sessions.session_n_friends) / first(col('session_hours'))).alias('session_avg_friends'),\n",
    "        (avg(df_sessions.session_n_add_playlist) / first(col('session_hours'))).alias('session_avg_added_playlist'),\n",
    "        (avg(df_sessions.session_n_home) / first(col('session_hours'))).alias('session_avg_home'),\n",
    "        (avg(df_sessions.session_n_ads) / first(col('session_hours'))).alias('session_avg_ads'),\n",
    "        (avg(df_sessions.session_n_help) / first(col('session_hours'))).alias('session_avg_help'),\n",
    "        (avg(df_sessions.session_n_error) / first(col('session_hours'))).alias('session_avg_errors'),\n",
    "        (avg(df_sessions.session_n_sets) / first(col('session_hours'))).alias('session_avg_settings'),\n",
    "        (avg(df_sessions.session_n_actions) / first(col('session_hours'))).alias('session_avg_actions')\n",
    "    )\n",
    "    \n",
    "    return df_sessions\n",
    "\n",
    "def create_user_dimension(df):\n",
    "    \n",
    "    df_user_profile = df.groupby('userId')\\\n",
    "        .agg( \n",
    "\n",
    "            # first(col('state')).alias('state'),\n",
    "            first(when(col('gender') == 'M', TRUE).otherwise(FALSE)).alias('male'),\n",
    "\n",
    "            smin(col('first_ts')).alias('ts_start'),\n",
    "            smax(col('last_event_ts')).alias('ts_end'),        \n",
    "        \n",
    "            ((smax(col('last_event_ts')) - smin(col('first_ts'))) / milliseconds_to_hours).alias('time_window'),\n",
    "        \n",
    "            # Subscription\n",
    "            count(when(col('page') == 'Submit Downgrade', True)).alias('n_downgrades'),\n",
    "            count(when(col('page') == 'Submit Upgrade', True)).alias('n_upgrades'),\n",
    "            last(when(col('level') == 'paid', TRUE).otherwise(FALSE)).alias('paid'),\n",
    "            first(when(col('last_page') == CHURN_CANCELLATION_PAGE, TRUE).otherwise(FALSE)).alias('canceled'),\n",
    "\n",
    "            # Streaming\n",
    "            count(when(col('page') == 'NextSong', True)).alias('n_songs'),\n",
    "            count(when(col('page') == 'Thumbs Up', True)).alias('n_likes'),\n",
    "            count(when(col('page') == 'Thumbs Down', True)).alias('n_dislikes'),\n",
    "            countDistinct(col('sessionId')).alias('n_sess'),\n",
    "            (avg(col('session_duration')) / milliseconds_to_hours).alias('avg_session_duration'),\n",
    "\n",
    "            # Community\n",
    "            count(when(col('page') == 'Add Friend', True)).alias('n_friends'),\n",
    "            count(when(col('page') == 'Add to Playlist', True)).alias('n_added_to_playlist'),\n",
    "\n",
    "            # Other\n",
    "            count(when(col('page') == 'Home', True)).alias('n_home'),\n",
    "            count(when(col('page') == 'Roll Advert', True)).alias('n_ads'),\n",
    "            count(when(col('page') == 'Help', True)).alias('n_help'),\n",
    "            count(when(col('page') == 'Error', True)).alias('n_errors'),\n",
    "            count(when(col('page') == 'Settings', True)).alias('n_settings'),\n",
    "            count(col('page')).alias('n_actions')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Location\n",
    "    # states = list(map(lambda c: c[0].strip(), df.select(['state']).distinct().rdd.collect()))\n",
    "    # for state in states:\n",
    "    #    df_user_profile = df_user_profile.withColumn(state.lower(), when(df_user_profile.state == state, 1).otherwise(0))\n",
    "    \n",
    "    return df_user_profile\n",
    "\n",
    "def create_days_dimension(df):\n",
    "    \n",
    "    df_unique_days = df.groupby('userId').agg(countDistinct('date').alias('n_days'))\n",
    "    \n",
    "    df_daily_actions = df.groupby('userId', 'date').agg(count('page').alias('total'))\n",
    "    df_daily_actions = df_daily_actions.groupby('userId').agg(avg('total').alias('avg_daily_actions')) \n",
    "\n",
    "    df_days = df_unique_days.join(df_daily_actions, df_unique_days.userId == df_daily_actions.userId)\n",
    "    \n",
    "    # Remove duplicated column after join\n",
    "    df_days = df_days.drop(df_daily_actions.userId)\n",
    "    \n",
    "    return df_days\n",
    "\n",
    "def sort_features(df, columns_order):\n",
    "    _columns = df.columns\n",
    "    _columns.sort()\n",
    "    \n",
    "    for _idx, _val in list(enumerate(columns_order)):\n",
    "        _columns.pop(_columns.index(_val))\n",
    "        _columns.insert(_idx, _val)\n",
    "        \n",
    "    assert len(_columns) == len(df.columns)\n",
    "\n",
    "    return _columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:20:54.202205Z",
     "start_time": "2020-03-28T23:20:54.188526Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_features = [ 'paid', 'male' ]\n",
    "\n",
    "numeric_features = [\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_all = [\n",
    "    'canceled',\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_to_train = [\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:21:38.145031Z",
     "start_time": "2020-03-28T23:21:37.346525Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sessions = create_session_dimension(df)\n",
    "df_days = create_days_dimension(df)\n",
    "\n",
    "df_users = create_user_dimension(df)\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_days, on = 'userId')\n",
    "\n",
    "_columns = sort_features(df_users, [ 'userId', 'male', 'paid', 'canceled'])\n",
    "_columns = list(set(df_users.schema.names + df_sessions.schema.names) - set(['ts_start', 'ts_end', 'state']))\n",
    "\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:23:04.772721Z",
     "start_time": "2020-03-28T23:23:04.764684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canceled',\n",
       " 'male',\n",
       " 'paid',\n",
       " 'avg_daily_actions',\n",
       " 'avg_session_duration',\n",
       " 'n_actions',\n",
       " 'n_added_to_playlist',\n",
       " 'n_ads',\n",
       " 'n_days',\n",
       " 'n_dislikes',\n",
       " 'n_downgrades',\n",
       " 'n_errors',\n",
       " 'n_friends',\n",
       " 'n_help',\n",
       " 'n_home',\n",
       " 'n_likes',\n",
       " 'n_sess',\n",
       " 'n_settings',\n",
       " 'n_songs',\n",
       " 'n_upgrades',\n",
       " 'session_avg_actions',\n",
       " 'session_avg_added_playlist',\n",
       " 'session_avg_ads',\n",
       " 'session_avg_dislikes',\n",
       " 'session_avg_errors',\n",
       " 'session_avg_friends',\n",
       " 'session_avg_help',\n",
       " 'session_avg_home',\n",
       " 'session_avg_likes',\n",
       " 'session_avg_playback',\n",
       " 'session_avg_settings',\n",
       " 'session_avg_songs',\n",
       " 'session_avg_time_away',\n",
       " 'time_window']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:26:13.302026Z",
     "start_time": "2020-03-28T23:26:13.292199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canceled',\n",
       " 'male',\n",
       " 'paid',\n",
       " 'avg_daily_actions',\n",
       " 'avg_session_duration',\n",
       " 'n_actions',\n",
       " 'n_added_to_playlist',\n",
       " 'n_ads',\n",
       " 'n_days',\n",
       " 'n_dislikes',\n",
       " 'n_downgrades',\n",
       " 'n_errors',\n",
       " 'n_friends',\n",
       " 'n_help',\n",
       " 'n_home',\n",
       " 'n_likes',\n",
       " 'n_sess',\n",
       " 'n_settings',\n",
       " 'n_songs',\n",
       " 'n_upgrades',\n",
       " 'session_avg_actions',\n",
       " 'session_avg_added_playlist',\n",
       " 'session_avg_ads',\n",
       " 'session_avg_dislikes',\n",
       " 'session_avg_errors',\n",
       " 'session_avg_friends',\n",
       " 'session_avg_help',\n",
       " 'session_avg_home',\n",
       " 'session_avg_likes',\n",
       " 'session_avg_playback',\n",
       " 'session_avg_settings',\n",
       " 'session_avg_songs',\n",
       " 'session_avg_time_away',\n",
       " 'time_window']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:21:43.423483Z",
     "start_time": "2020-03-28T23:21:43.282737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "df_users = df_users.select(columns_all).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:27:05.625450Z",
     "start_time": "2020-03-28T23:26:50.225516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " canceled                   | 1   \n",
      " male                       | 0   \n",
      " paid                       | 0   \n",
      " avg_daily_actions          | 69  \n",
      " avg_session_duration       | 4   \n",
      " n_actions                  | 137 \n",
      " n_added_to_playlist        | 1   \n",
      " n_ads                      | 22  \n",
      " n_days                     | 2   \n",
      " n_dislikes                 | 3   \n",
      " n_downgrades               | 0   \n",
      " n_errors                   | 0   \n",
      " n_friends                  | 3   \n",
      " n_help                     | 0   \n",
      " n_home                     | 3   \n",
      " n_likes                    | 4   \n",
      " n_sess                     | 2   \n",
      " n_settings                 | 0   \n",
      " n_songs                    | 96  \n",
      " n_upgrades                 | 0   \n",
      " session_avg_actions        | 20  \n",
      " session_avg_added_playlist | 0   \n",
      " session_avg_ads            | 3   \n",
      " session_avg_dislikes       | 0   \n",
      " session_avg_errors         | 0   \n",
      " session_avg_friends        | 0   \n",
      " session_avg_help           | 0   \n",
      " session_avg_home           | 0   \n",
      " session_avg_likes          | 1   \n",
      " session_avg_playback       | 1   \n",
      " session_avg_settings       | 0   \n",
      " session_avg_songs          | 14  \n",
      " session_avg_time_away      | 66  \n",
      " time_window                | 73  \n",
      "-RECORD 1-------------------------\n",
      " canceled                   | 1   \n",
      " male                       | 1   \n",
      " paid                       | 0   \n",
      " avg_daily_actions          | 66  \n",
      " avg_session_duration       | 6   \n",
      " n_actions                  | 395 \n",
      " n_added_to_playlist        | 6   \n",
      " n_ads                      | 11  \n",
      " n_days                     | 6   \n",
      " n_dislikes                 | 5   \n",
      " n_downgrades               | 0   \n",
      " n_errors                   | 0   \n",
      " n_friends                  | 2   \n",
      " n_help                     | 1   \n",
      " n_home                     | 23  \n",
      " n_likes                    | 15  \n",
      " n_sess                     | 5   \n",
      " n_settings                 | 2   \n",
      " n_songs                    | 310 \n",
      " n_upgrades                 | 1   \n",
      " session_avg_actions        | 18  \n",
      " session_avg_added_playlist | 0   \n",
      " session_avg_ads            | 1   \n",
      " session_avg_dislikes       | 0   \n",
      " session_avg_errors         | 0   \n",
      " session_avg_friends        | 0   \n",
      " session_avg_help           | 0   \n",
      " session_avg_home           | 1   \n",
      " session_avg_likes          | 1   \n",
      " session_avg_playback       | 1   \n",
      " session_avg_settings       | 0   \n",
      " session_avg_songs          | 14  \n",
      " session_avg_time_away      | 167 \n",
      " time_window                | 690 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WARN: Only round to display\n",
    "# Enforces the order for some columns\n",
    "df_users.select([sround(c, 0).cast(dataType = IntegerType()).alias(c) for c in columns_all]).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T23:28:03.064985Z",
     "start_time": "2020-03-28T23:27:50.571795Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.select(columns_all).fillna(0).toPandas().to_csv('sparkify_data_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:53.790091Z",
     "start_time": "2020-03-21T20:49:53.293466Z"
    }
   },
   "outputs": [],
   "source": [
    "df.agg(countDistinct(df.userId).alias('unique_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).groupBy('canceled').agg(count(df_users.canceled).alias('total')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advertises number (per session and all)\n",
    "    - The user **100010** returned after some idle time and received a considerable amount of advertises;\n",
    "    - Also, after thumbs down, I received two advertisements on four sounds. Then canceled the service.\n",
    "- Number of sessions\n",
    "- Paid subscription time\n",
    "- Avg songs before an ad\n",
    "- Number of skipped songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_date(df.ts.cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.userId == user_id).select(['artist',\n",
    " 'auth',\n",
    " 'firstName',\n",
    " 'gender',\n",
    " 'itemInSession',\n",
    " 'lastName',\n",
    " 'length',\n",
    " 'level', \n",
    " 'page',\n",
    " 'sessionId',\n",
    " 'song', \n",
    " 'ts', \n",
    " 'userId']).orderBy('sessionId', 'itemInSession').withColumn('datetime', date_format((df.ts/1000).cast(dataType=TimestampType()), 'HH:mm:ss dd-MM-YYYY')).show(350, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:01:34.745712Z",
     "start_time": "2020-03-01T15:01:34.737694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n",
      "\n",
      "Columns to train: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n"
     ]
    }
   ],
   "source": [
    "columns_to_exclude = set(['userId'])\n",
    "\n",
    "columns_to_use = list(set(df_users.columns) - columns_to_exclude)\n",
    "\n",
    "columns_to_train = list(set(columns_to_use) - set(['canc']))\n",
    "\n",
    "columns_to_use.sort()\n",
    "columns_to_train.sort()\n",
    "\n",
    "print(f'Columns: {columns_to_use}\\n')\n",
    "print(f'Columns to train: {columns_to_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T02:49:38.697200Z",
     "start_time": "2020-03-25T02:49:38.693359Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_LABEL = 'canceled'\n",
    "TRAIN_SPLIT_RATIO = .7\n",
    "TEST_SPLIT_RATIO = .3\n",
    "\n",
    "SPLIT_RATIO = [TRAIN_SPLIT_RATIO, TEST_SPLIT_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T02:49:44.551597Z",
     "start_time": "2020-03-25T02:49:44.533296Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_predictions):\n",
    "    \n",
    "    # auc = roc_auc_score(y_test, y_predictions)\n",
    "    cm = confusion_matrix(y_test, y_predictions, labels = [1, 0])\n",
    "    \n",
    "    tn = cm[0, 0]\n",
    "    tp = cm[1, 1]\n",
    "    fn = cm[1, 0]\n",
    "    fp = cm[0, 1]\n",
    "    \n",
    "    total = np.sum(cm) # tn + tp + fn + fp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn) \n",
    "    \n",
    "    print(cm)\n",
    "\n",
    "def evaluate_multiclass_classifier(predictions, columns):\n",
    "    metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall' ]\n",
    "    \n",
    "    result = {}\n",
    "    for metric in metrics_to_evaluate:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol = columns[0], predictionCol = columns[1], metricName = metric)\n",
    "        value = evaluator.evaluate(predictions)\n",
    "        result[metric] = value\n",
    "        print(f'{metric}: {value}') \n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_random_forest_classifier(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10, maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42)\n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, rf_classifier, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "    \n",
    "\n",
    "def create_pipeline(model):\n",
    "    \n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, model ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def create_random_forest_pipeline():\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'canceled', featuresCol = 'features', seed = 42)\n",
    "    return create_pipeline(rf_classifier)\n",
    "\n",
    "def create_gradient_boost_pipeline():\n",
    "    gbt_classifier = GBTClassifier(labelCol = 'canceled', maxDepth = 5, maxIter = 100, seed = 42)\n",
    "    return create_pipeline(gbt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:07:01.999691Z",
     "start_time": "2020-03-21T21:06:53.158970Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:14:23.154867Z",
     "start_time": "2020-03-21T23:13:27.220280Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:15:33.676310Z",
     "start_time": "2020-03-21T23:14:48.782886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8257575757575758\n",
      "f1: 0.7947522154088501\n",
      "weightedPrecision: 0.8269570707070706\n",
      "weightedRecall: 0.8257575757575758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8257575757575758,\n",
       " 'f1': 0.7947522154088501,\n",
       " 'weightedPrecision': 0.8269570707070706,\n",
       " 'weightedRecall': 0.8257575757575758}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_multiclass_classifier(predictions, ('canceled', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:15:59.264578Z",
     "start_time": "2020-03-21T23:15:49.342154Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction', 'predictedLabel']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:54:56.121279Z",
     "start_time": "2020-03-21T21:54:56.113194Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:38:50.323684Z",
     "start_time": "2020-03-21T21:38:50.316950Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T02:50:10.781546Z",
     "start_time": "2020-03-25T02:50:10.772888Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:27:07.966315Z",
     "start_time": "2020-02-29T21:26:10.470241Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_to_use, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:49:02.428480Z",
     "start_time": "2020-02-29T21:47:25.148792Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(predictions, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:49:40.743405Z",
     "start_time": "2020-02-29T21:49:22.720697Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:56:27.866180Z",
     "start_time": "2020-03-22T02:56:27.860716Z"
    }
   },
   "outputs": [],
   "source": [
    "list(range(5, 45, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T02:50:22.764840Z",
     "start_time": "2020-03-25T02:50:22.749352Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_grid_search(pipeline, param_grid, evaluator = FNR(labelCol = 'canceled')):\n",
    "    \n",
    "    return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = evaluator, numFolds = 3, parallelism = 16, seed = 42)\n",
    "\n",
    "def random_forest_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_rf = ParamGridBuilder().addGrid(model.maxDepth, [5, 10, 15, 20, 25]) \n",
    "    grid_rf = grid_rf.addGrid(model.impurity, ['gini']) \n",
    "    grid_rf = grid_rf.addGrid(model.maxBins, [5, 10, 15, 20, 25, 30, 35, 40])\n",
    "    grid_rf = grid_rf.addGrid(model.numTrees, [10, 20, 40, 60, 70])\n",
    "    grid_rf = grid_rf.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_rf)}')\n",
    "        \n",
    "    return create_grid_search(pipeline, grid_rf)\n",
    "\n",
    "def gradient_boost_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_gbt = ParamGridBuilder().addGrid(model.maxDepth, [2, 4, 6, 8, 10])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxIter, [20, 25, 40, 50, 100])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxBins, [2])\n",
    "    grid_gbt = grid_gbt.addGrid(model.subsamplingRate, [.5, .8, 1])\n",
    "    grid_gbt = grid_gbt.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_gbt)}')\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T02:53:24.068722Z",
     "start_time": "2020-03-25T02:53:24.051653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "# data = df_users.select(columns_to_use).fillna(0)\n",
    "\n",
    "# Split train/test\n",
    "(train_df, test_df) = df_users.randomSplit(SPLIT_RATIO, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T16:48:02.519957Z",
     "start_time": "2020-03-22T16:06:46.868617Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)\n",
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:54:34.464036Z",
     "start_time": "2020-03-22T20:06:54.143404Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = gradient_boost_grid_search(pipeline)\n",
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:59:20.795034Z",
     "start_time": "2020-03-22T20:59:20.790105Z"
    }
   },
   "outputs": [],
   "source": [
    "min(cv_rf_results.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:55:41.490417Z",
     "start_time": "2020-03-22T20:55:10.991439Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T10:40:06.568799Z",
     "start_time": "2020-03-23T02:44:33.041437Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T14:16:38.672206Z",
     "start_time": "2020-03-25T02:53:27.924737Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T02:30:06.055126Z",
     "start_time": "2020-03-23T02:29:45.063311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 12]\n",
      " [ 5 96]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T02:36:38.462297Z",
     "start_time": "2020-03-23T02:36:17.313559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 12]\n",
      " [ 5 96]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T21:08:46.534650Z",
     "start_time": "2020-03-22T21:03:03.685688Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T21:13:15.171512Z",
     "start_time": "2020-03-22T21:12:28.885280Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T22:57:17.217600Z",
     "start_time": "2020-03-22T22:57:17.212592Z"
    }
   },
   "outputs": [],
   "source": [
    "_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T01:40:00.733556Z",
     "start_time": "2020-03-22T23:37:41.846480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 25\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T01:43:46.683262Z",
     "start_time": "2020-03-23T01:43:24.302234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21 10]\n",
      " [12 89]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T02:01:07.081948Z",
     "start_time": "2020-03-23T02:01:07.074972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_ba2654d5e488', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'all',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='labelCol', doc='label column name'): 'canceled',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.'): 32,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxDepth', doc='Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxIter', doc='maximum number of iterations (>= 0)'): 20,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.'): 1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='seed', doc='random seed'): -6852653717611865011,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T01:52:18.585582Z",
     "start_time": "2020-03-23T01:52:18.579643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='GBTClassifier_ba2654d5e488', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(_results.avgMetrics, _results.getEstimatorParamMaps()))[0][1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T03:52:21.429764Z",
     "start_time": "2020-03-22T03:08:58.813753Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)\n",
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T17:10:35.987456Z",
     "start_time": "2020-03-22T17:10:10.484596Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T19:31:10.200540Z",
     "start_time": "2020-03-22T19:31:10.189599Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:00:40.446125Z",
     "start_time": "2020-03-22T19:59:39.685736Z"
    }
   },
   "outputs": [],
   "source": [
    "FNR(predictionCol = \"prediction\", labelCol=\"idx_labels\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T17:27:25.837073Z",
     "start_time": "2020-03-22T17:27:25.831381Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results.bestModel.stages[-1].numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10)\n",
    "dir(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T17:11:17.409191Z",
     "start_time": "2020-03-22T17:11:17.403489Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:43:55.011960Z",
     "start_time": "2020-03-21T22:43:45.931376Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:44:06.060150Z",
     "start_time": "2020-03-21T22:43:56.785468Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:11:05.946121Z",
     "start_time": "2020-03-21T22:10:56.684420Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:07:09.309168Z",
     "start_time": "2020-03-21T22:07:09.304265Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    " \n",
    "best_model_results = cv_gbt_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:02:01.341210Z",
     "start_time": "2020-03-01T15:02:01.298441Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:08:34.371481Z",
     "start_time": "2020-03-01T15:02:03.210992Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_gbt_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:39.482836Z",
     "start_time": "2020-03-01T14:06:39.477907Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_gbt_results.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:56.517560Z",
     "start_time": "2020-03-01T14:06:56.488834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = cv_gbt_results.avgMetrics\n",
    "params = [{p.name: v for p, v in m.items()} for m in cv_gbt.getEstimatorParamMaps()]\n",
    "params_pd = pd.DataFrame(params)\n",
    "params_pd['score'] = scores\n",
    "params_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:00:50.268594Z",
     "start_time": "2020-03-01T03:00:44.103376Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    " \n",
    "best_model_results = cv_gbt_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:35:59.822421Z",
     "start_time": "2020-03-21T23:35:39.991922Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluator = BinaryClassificationEvaluator(labelCol = 'canceled', metricName = 'f1Measure')\n",
    "\n",
    "metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall' ]\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'canceled', metricName = 'f1')\n",
    " \n",
    "best_model_results = cv_rf_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:01:23.359069Z",
     "start_time": "2020-03-01T03:01:19.905831Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(best_model_results, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:39:49.645897Z",
     "start_time": "2020-03-01T00:38:10.913242Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(best_model_results, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:13:53.940084Z",
     "start_time": "2020-03-01T00:13:40.288881Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(['features', 'prediction', 'canc']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T22:56:07.000157Z",
     "start_time": "2020-02-29T22:55:53.705530Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(['rawPrediction', 'prediction', 'canc']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:06:14.995253Z",
     "start_time": "2020-02-29T21:05:57.745177Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.filter('canc = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:06:44.651611Z",
     "start_time": "2020-02-29T21:06:27.509694Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.filter('canc = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:01:53.168730Z",
     "start_time": "2020-03-01T03:01:52.310500Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(\"prediction\", \"canc\", \"features\").filter('canc = 1').groupby(['canc', 'prediction']).agg({'canc':'count'}).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:47:05.176261Z",
     "start_time": "2020-03-01T00:47:05.169056Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
