{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T21:22:48.405068Z",
     "start_time": "2020-05-10T21:22:48.400749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "# findspark.init('/home/brunowdev/spark-2.4.5-bin-hadoop2.6/')\n",
    "\n",
    "findspark.init('/home/bruno/LIBS/spark')\n",
    "\n",
    "import evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T21:22:49.859543Z",
     "start_time": "2020-05-10T21:22:49.270779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import min as smin, max as smax, sum as ssum, round as sround, abs as sabs, pow as spow\n",
    "from pyspark.sql.functions import isnan, isnull, when, first, avg, sqrt, last, count, countDistinct, col, lag, lead, coalesce, lit, split, trim\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_date, date_format, from_unixtime, to_timestamp\n",
    "\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    " \n",
    "import jupyter_utils as j\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, MaxAbsScaler, Normalizer, MinMaxScaler, StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, Evaluator\n",
    "from pyspark import since, keyword_only\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T21:22:52.795807Z",
     "start_time": "2020-05-10T21:22:52.789981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j.reload(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T21:22:53.863488Z",
     "start_time": "2020-05-10T21:22:53.853924Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:36:01.552248Z",
     "start_time": "2020-05-07T23:36:01.526830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1588894223744'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '35575'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.driver.host', '192.168.0.102'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T21:33:05.105416Z",
     "start_time": "2020-05-10T21:33:05.098221Z"
    }
   },
   "outputs": [],
   "source": [
    "def spark_read(spark):\n",
    "    return spark.read.option('inferSchema', 'true').option('header', 'true').option('encoding', 'utf-8')\n",
    "\n",
    "def load_medium_json_file(spark):\n",
    "    df = spark_read(spark).json('medium_sparkify_event_data.json')\n",
    "    return df\n",
    "    \n",
    "def load_full_json_file(spark):\n",
    "    df = spark_read(spark).json('full_sparkify_event_data.json')\n",
    "    return df\n",
    "\n",
    "def load_full_csv_file(spark):\n",
    "    df = spark_read(spark).csv('sparkify_full_csv_data.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:36:23.935605Z",
     "start_time": "2020-05-07T23:36:12.335340Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").csv(filepath)\n",
    "# df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").json(filepath)\n",
    "\n",
    "df = load_full_csv_file(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:36:24.003769Z",
     "start_time": "2020-05-07T23:36:23.936866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gender: string, length: double, level: string, registration: double, userId: int, ts: bigint, page: string, sessionId: int, itemInSession: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:45:26.034576Z",
     "start_time": "2020-05-07T23:45:00.318017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "|gender|length|level|registration|userId| ts|page|sessionId|itemInSession|\n",
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.userId == 100010).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:45:26.048913Z",
     "start_time": "2020-05-07T23:45:26.037086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger instance created\n"
     ]
    }
   ],
   "source": [
    "log4jLogger = spark.sparkContext._jvm.org.apache.log4j\n",
    "\n",
    "LOGGER = log4jLogger.LogManager.getLogger('driver_logger')\n",
    "\n",
    "def info(message, print_on_notebook = True):\n",
    "    LOGGER.info(message)\n",
    "    \n",
    "    if print_on_notebook:\n",
    "        print(message)\n",
    "    \n",
    "info('Logger instance created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:46:00.161979Z",
     "start_time": "2020-05-07T23:46:00.153307Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "def set_storage_on_memory(df):\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)\n",
    "    \n",
    "set_storage_on_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:53:34.680286Z",
     "start_time": "2020-05-03T18:53:34.674338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:46:26.556386Z",
     "start_time": "2020-05-07T23:46:26.543421Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_CANCELLATION_PAGE = 'Cancellation Confirmation'\n",
    "REGISTRATION_PAGE = 'Submit Registration'\n",
    "milliseconds_to_hours = 3600 * 1000\n",
    "minutes_to_hours = 60 * 60\n",
    "TRUE = 1\n",
    "FALSE = 0\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \n",
    "    info('Starting data cleaning...')\n",
    "    \n",
    "    total_before = df.count()\n",
    "    \n",
    "    # Keep only logged records\n",
    "    # df = df.where(df.auth.isin(['Logged In', 'Cancelled']))\n",
    "    \n",
    "    # Records without userId\n",
    "    df = df.where(col('userId').isNotNull())\n",
    "    \n",
    "    # Create a date column for the event\n",
    "    df = df.withColumn('date', from_unixtime(col('ts') / 1000).cast(DateType()))\n",
    "    \n",
    "    # Relevant windows\n",
    "    w_session = Window.partitionBy('sessionId').orderBy('ts')\n",
    "    w_user_session = Window.partitionBy('sessionId', 'userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    w_user = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # Create features\n",
    "    df = df.withColumn('previous_page', lag(df.page).over(w_session))\n",
    "    df = df.withColumn('last_event_ts', last(col('ts')).over(w_user))\n",
    "    df = df.withColumn('last_page', last(col('page')).over(w_user))\n",
    "    df = df.withColumn('register_page', first(col('previous_page')).over(w_user))\n",
    "    df = df.withColumn('first_ts', first(col('ts')).over(w_user))\n",
    "    df = df.withColumn('ts_elapsed', last(df.ts).over(w_session) - first(df.ts).over(w_user_session))\n",
    "    df = df.withColumn('session_duration', smax(df.ts_elapsed).over(w_user_session))\n",
    "     \n",
    "    info('Finished data cleaning...')\n",
    "    info(f'Number of removed rows: {total_before - df.count()}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:46:30.828353Z",
     "start_time": "2020-05-07T23:46:29.871552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Finished data cleaning...\n",
      "Number of removed rows: 0\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:46:36.335422Z",
     "start_time": "2020-05-07T23:46:34.919231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                page|   count|\n",
      "+--------------------+--------+\n",
      "|            NextSong|20850272|\n",
      "|                Home| 1343102|\n",
      "|           Thumbs Up| 1151465|\n",
      "|     Add to Playlist|  597921|\n",
      "|         Roll Advert|  385212|\n",
      "|          Add Friend|  381664|\n",
      "|               Login|  296350|\n",
      "|              Logout|  296005|\n",
      "|         Thumbs Down|  239212|\n",
      "|           Downgrade|  184240|\n",
      "|                Help|  155100|\n",
      "|            Settings|  147074|\n",
      "|               About|   92759|\n",
      "|             Upgrade|   50507|\n",
      "|       Save Settings|   29516|\n",
      "|               Error|   25962|\n",
      "|      Submit Upgrade|   15135|\n",
      "|    Submit Downgrade|    6494|\n",
      "|              Cancel|    5003|\n",
      "|Cancellation Conf...|    5003|\n",
      "|            Register|     802|\n",
      "| Submit Registration|     401|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions about the data:\n",
    "\n",
    "- Are errors related to downgrading canceling the service?\n",
    "- Having a certain number of friends or a sense of community can decrease the churn?\n",
    "- Thumbs down are related to churn? (could the quality of the songs catalog affect the churn)\n",
    "- The advertising is not annoying the users?\n",
    "- Users with stay connected for more time have less change to churn?\n",
    "- Is the home page relevant?\n",
    "- Users, who access the downgrade page are how much more willing to churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:06.259628Z",
     "start_time": "2020-03-08T18:30:05.841058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('status').count().orderBy('count', ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T22:44:01.657485Z",
     "start_time": "2020-03-01T22:44:01.094695Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:16.616860Z",
     "start_time": "2020-03-08T18:30:16.073824Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:29.650593Z",
     "start_time": "2020-03-08T18:30:29.435808Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('userAgent').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:37.323100Z",
     "start_time": "2020-03-08T18:30:36.828209Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92 and song != \\'null\\' ').groupBy('song').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:50:50.595782Z",
     "start_time": "2020-05-07T23:50:50.563569Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_session_dimension(df):\n",
    "    \n",
    "    # sessions from the user\n",
    "    df_sessions = df.orderBy(df.sessionId).groupBy('sessionId', 'userId').agg(\n",
    "        smax(df.ts).alias('max_event_ts'),\n",
    "        smin(df.ts).alias('min_event_ts'),\n",
    "        ssum(df.length).alias('session_n_total_playback'), # Based on songs length\n",
    "        count(when(df.page == 'Thumbs Up', True)).alias(\"session_n_likes\"),\n",
    "        count(when(df.page == 'Thumbs Down', True)).alias(\"session_n_dislikes\"),\n",
    "        count(when(df.page == 'NextSong', True)).alias(\"session_n_songs\"),\n",
    "        count(when(df.page == 'Add Friend', True)).alias(\"session_n_friends\"),\n",
    "        count(when(df.page == 'Add to Playlist', True)).alias(\"session_n_add_playlist\"),\n",
    "        count(when(df.page == 'Home', True)).alias(\"session_n_home\"),\n",
    "        count(when(df.page == 'Roll Advert', True)).alias(\"session_n_ads\"),\n",
    "        count(when(df.page == 'Help', True)).alias(\"session_n_help\"),\n",
    "        count(when(df.page == 'Error', True)).alias(\"session_n_error\"),\n",
    "        count(when(df.page == 'Settings', True)).alias(\"session_n_sets\"),\n",
    "        count(col('page')).alias('session_n_actions'),\n",
    "        first(col('session_duration')).alias('session_duration')\n",
    "    ) \n",
    "    \n",
    "    # Calculate the interval until the next session\n",
    "    w_user_sessions_interval = Window.partitionBy('userId').orderBy('min_event_ts')\n",
    "    df_sessions = df_sessions.withColumn('interval_to_session', col('min_event_ts') - lag(col('max_event_ts')).over(w_user_sessions_interval))\n",
    "    \n",
    "    # Calculate average time in hours for each session\n",
    "    df_session_time = df_sessions.groupBy('userId').agg(\n",
    "       # (avg(df_sessions.session_duration) / milliseconds_to_hours).alias('session_hours')\n",
    "        (avg(df_sessions.session_n_total_playback) / minutes_to_hours).alias('session_hours')\n",
    "    )\n",
    "    df_sessions = df_sessions.join(df_session_time, on = 'userId')\n",
    "    \n",
    "    # We should remove the null lines before count/group to not account 2 times the mean interval\n",
    "    df_sessions = df_sessions.groupBy('userId').agg(  \n",
    "        (avg(df_sessions.interval_to_session) / milliseconds_to_hours).alias('session_avg_time_away'),\n",
    "        ((avg(df_sessions.session_n_total_playback) / minutes_to_hours) / first(col('session_hours'))).alias('session_avg_playback'), \n",
    "        (avg(df_sessions.session_n_likes) / first(col('session_hours'))).alias('session_avg_likes'),\n",
    "        (avg(df_sessions.session_n_dislikes) / first(col('session_hours'))).alias('session_avg_dislikes'),\n",
    "        (avg(df_sessions.session_n_songs) / first(col('session_hours'))).alias('session_avg_songs'),\n",
    "        (avg(df_sessions.session_n_friends) / first(col('session_hours'))).alias('session_avg_friends'),\n",
    "        (avg(df_sessions.session_n_add_playlist) / first(col('session_hours'))).alias('session_avg_added_playlist'),\n",
    "        (avg(df_sessions.session_n_home) / first(col('session_hours'))).alias('session_avg_home'),\n",
    "        (avg(df_sessions.session_n_ads) / first(col('session_hours'))).alias('session_avg_ads'),\n",
    "        (avg(df_sessions.session_n_help) / first(col('session_hours'))).alias('session_avg_help'),\n",
    "        (avg(df_sessions.session_n_error) / first(col('session_hours'))).alias('session_avg_errors'),\n",
    "        (avg(df_sessions.session_n_sets) / first(col('session_hours'))).alias('session_avg_settings'),\n",
    "        (avg(df_sessions.session_n_actions) / first(col('session_hours'))).alias('session_avg_actions')\n",
    "    )\n",
    "    \n",
    "    return df_sessions\n",
    "\n",
    "def create_user_dimension(df):\n",
    "    \n",
    "    df_user_profile = df.groupby('userId')\\\n",
    "        .agg( \n",
    "\n",
    "            # first(col('state')).alias('state'),\n",
    "            first(when(col('gender') == 'M', TRUE).otherwise(FALSE)).alias('male'),\n",
    "\n",
    "            smin(col('first_ts')).alias('ts_start'),\n",
    "            smax(col('last_event_ts')).alias('ts_end'),        \n",
    "        \n",
    "            ((smax(col('last_event_ts')) - smin(col('first_ts'))) / milliseconds_to_hours).alias('time_window'),\n",
    "        \n",
    "            # Subscription\n",
    "            count(when(col('page') == 'Submit Downgrade', True)).alias('n_downgrades'),\n",
    "            count(when(col('page') == 'Submit Upgrade', True)).alias('n_upgrades'),\n",
    "            last(when(col('level') == 'paid', TRUE).otherwise(FALSE)).alias('paid'),\n",
    "            first(when(col('last_page') == CHURN_CANCELLATION_PAGE, TRUE).otherwise(FALSE)).alias('canceled'),\n",
    "\n",
    "            # Streaming\n",
    "            count(when(col('page') == 'NextSong', True)).alias('n_songs'),\n",
    "            count(when(col('page') == 'Thumbs Up', True)).alias('n_likes'),\n",
    "            count(when(col('page') == 'Thumbs Down', True)).alias('n_dislikes'),\n",
    "            countDistinct(col('sessionId')).alias('n_sess'),\n",
    "            (avg(col('session_duration')) / milliseconds_to_hours).alias('avg_session_duration'),\n",
    "\n",
    "            # Community\n",
    "            count(when(col('page') == 'Add Friend', True)).alias('n_friends'),\n",
    "            count(when(col('page') == 'Add to Playlist', True)).alias('n_added_to_playlist'),\n",
    "\n",
    "            # Other\n",
    "            count(when(col('page') == 'Home', True)).alias('n_home'),\n",
    "            count(when(col('page') == 'Roll Advert', True)).alias('n_ads'),\n",
    "            count(when(col('page') == 'Help', True)).alias('n_help'),\n",
    "            count(when(col('page') == 'Error', True)).alias('n_errors'),\n",
    "            count(when(col('page') == 'Settings', True)).alias('n_settings'),\n",
    "            count(col('page')).alias('n_actions')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Location\n",
    "    # states = list(map(lambda c: c[0].strip(), df.select(['state']).distinct().rdd.collect()))\n",
    "    # for state in states:\n",
    "    #    df_user_profile = df_user_profile.withColumn(state.lower(), when(df_user_profile.state == state, 1).otherwise(0))\n",
    "    \n",
    "    return df_user_profile\n",
    "\n",
    "def create_days_dimension(df):\n",
    "    \n",
    "    df_unique_days = df.groupby('userId').agg(countDistinct('date').alias('n_days'))\n",
    "    \n",
    "    df_daily_actions = df.groupby('userId', 'date').agg(count('page').alias('total'))\n",
    "    df_daily_actions = df_daily_actions.groupby('userId').agg(avg('total').alias('avg_daily_actions')) \n",
    "\n",
    "    df_days = df_unique_days.join(df_daily_actions, df_unique_days.userId == df_daily_actions.userId)\n",
    "    \n",
    "    # Remove duplicated column after join\n",
    "    df_days = df_days.drop(df_daily_actions.userId)\n",
    "    \n",
    "    return df_days\n",
    "\n",
    "def sort_features(df, columns_order):\n",
    "    _columns = df.columns\n",
    "    _columns.sort()\n",
    "    \n",
    "    for _idx, _val in list(enumerate(columns_order)):\n",
    "        _columns.pop(_columns.index(_val))\n",
    "        _columns.insert(_idx, _val)\n",
    "        \n",
    "    assert len(_columns) == len(df.columns)\n",
    "\n",
    "    return _columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:50:56.454172Z",
     "start_time": "2020-05-07T23:50:56.441826Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_features = [ 'paid', 'male' ]\n",
    "\n",
    "numeric_features = [\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_all = [\n",
    "    'canceled',\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_to_train = [\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data - create a unique row per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:50:59.202732Z",
     "start_time": "2020-05-07T23:50:58.513388Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sessions = create_session_dimension(df)\n",
    "df_days = create_days_dimension(df)\n",
    "\n",
    "df_users = create_user_dimension(df)\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_days, on = 'userId')\n",
    "\n",
    "_columns = sort_features(df_users, [ 'userId', 'male', 'paid', 'canceled'])\n",
    "_columns = list(set(df_users.schema.names + df_sessions.schema.names) - set(['ts_start', 'ts_end', 'state']))\n",
    "\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:51:04.165140Z",
     "start_time": "2020-05-07T23:51:04.041777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "df_users = df_users.select(columns_all).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:51:05.145394Z",
     "start_time": "2020-05-07T23:51:05.136078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- canceled: integer (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- avg_daily_actions: double (nullable = false)\n",
      " |-- avg_session_duration: double (nullable = false)\n",
      " |-- n_actions: long (nullable = false)\n",
      " |-- n_added_to_playlist: long (nullable = false)\n",
      " |-- n_ads: long (nullable = false)\n",
      " |-- n_days: long (nullable = false)\n",
      " |-- n_dislikes: long (nullable = false)\n",
      " |-- n_downgrades: long (nullable = false)\n",
      " |-- n_errors: long (nullable = false)\n",
      " |-- n_friends: long (nullable = false)\n",
      " |-- n_help: long (nullable = false)\n",
      " |-- n_home: long (nullable = false)\n",
      " |-- n_likes: long (nullable = false)\n",
      " |-- n_sess: long (nullable = false)\n",
      " |-- n_settings: long (nullable = false)\n",
      " |-- n_songs: long (nullable = false)\n",
      " |-- n_upgrades: long (nullable = false)\n",
      " |-- session_avg_actions: double (nullable = false)\n",
      " |-- session_avg_added_playlist: double (nullable = false)\n",
      " |-- session_avg_ads: double (nullable = false)\n",
      " |-- session_avg_dislikes: double (nullable = false)\n",
      " |-- session_avg_errors: double (nullable = false)\n",
      " |-- session_avg_friends: double (nullable = false)\n",
      " |-- session_avg_help: double (nullable = false)\n",
      " |-- session_avg_home: double (nullable = false)\n",
      " |-- session_avg_likes: double (nullable = false)\n",
      " |-- session_avg_playback: double (nullable = false)\n",
      " |-- session_avg_settings: double (nullable = false)\n",
      " |-- session_avg_songs: double (nullable = false)\n",
      " |-- session_avg_time_away: double (nullable = false)\n",
      " |-- time_window: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:51:09.726684Z",
     "start_time": "2020-05-07T23:51:09.120951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:00.780957Z",
     "start_time": "2020-05-07T23:51:11.054200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " canceled                   | 0                   \n",
      " male                       | 1                   \n",
      " paid                       | 1                   \n",
      " avg_daily_actions          | 56.833333333333336  \n",
      " avg_session_duration       | 8.064825676115998   \n",
      " n_actions                  | 682                 \n",
      " n_added_to_playlist        | 20                  \n",
      " n_ads                      | 3                   \n",
      " n_days                     | 12                  \n",
      " n_dislikes                 | 10                  \n",
      " n_downgrades               | 1                   \n",
      " n_errors                   | 3                   \n",
      " n_friends                  | 12                  \n",
      " n_help                     | 5                   \n",
      " n_home                     | 27                  \n",
      " n_likes                    | 25                  \n",
      " n_sess                     | 9                   \n",
      " n_settings                 | 4                   \n",
      " n_songs                    | 557                 \n",
      " n_upgrades                 | 0                   \n",
      " session_avg_actions        | 17.377708646219023  \n",
      " session_avg_added_playlist | 0.5096102242292969  \n",
      " session_avg_ads            | 0.07644153363439453 \n",
      " session_avg_dislikes       | 0.25480511211464846 \n",
      " session_avg_errors         | 0.07644153363439453 \n",
      " session_avg_friends        | 0.3057661345375781  \n",
      " session_avg_help           | 0.12740255605732423 \n",
      " session_avg_home           | 0.6879738027095508  \n",
      " session_avg_likes          | 0.6370127802866211  \n",
      " session_avg_playback       | 1.0                 \n",
      " session_avg_settings       | 0.10192204484585937 \n",
      " session_avg_songs          | 14.192644744785918  \n",
      " session_avg_time_away      | 152.10534722222224  \n",
      " time_window                | 1255.4402777777777  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.show(1, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:30.106693Z",
     "start_time": "2020-05-07T23:52:30.094770Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "import sys\n",
    "\n",
    "w = Window().partitionBy()\n",
    "\n",
    "def z_score(col, w):\n",
    "    _avg_ = avg(col).over(w)\n",
    "    avg_sq = avg(spow(col, 2)).over(w)\n",
    "    sd_ = sqrt(avg_sq - spow(_avg_, 2))\n",
    "    return sabs((col - _avg_) / sd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:32.690416Z",
     "start_time": "2020-05-07T23:52:32.685282Z"
    }
   },
   "outputs": [],
   "source": [
    " _columns_to_check_outliers = [ 'avg_daily_actions', 'avg_session_duration',  'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:34.231425Z",
     "start_time": "2020-05-07T23:52:33.846841Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in _columns_to_check_outliers:\n",
    "    df_users = df_users.withColumn(f'zscore_{c}', z_score(col(c), w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:35.430333Z",
     "start_time": "2020-05-07T23:52:35.427310Z"
    }
   },
   "outputs": [],
   "source": [
    "zscore_columns = []\n",
    "\n",
    "for c in _columns_to_check_outliers:\n",
    "    zscore_columns.append(f'zscore_{c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:36.537629Z",
     "start_time": "2020-05-07T23:52:36.532246Z"
    }
   },
   "outputs": [],
   "source": [
    "_query = ''\n",
    "_threshold = 3\n",
    "\n",
    "for c in zscore_columns:\n",
    "    _begin = ' and ' if len(_query) > 0 else ''\n",
    "    _query += f'{_begin}{c} < {_threshold}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:39.150452Z",
     "start_time": "2020-05-07T23:52:39.146808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zscore_avg_daily_actions < 3 and zscore_avg_session_duration < 3 and zscore_session_avg_actions < 3 and zscore_session_avg_added_playlist < 3 and zscore_session_avg_ads < 3 and zscore_session_avg_dislikes < 3 and zscore_session_avg_errors < 3 and zscore_session_avg_friends < 3 and zscore_session_avg_help < 3 and zscore_session_avg_home < 3 and zscore_session_avg_likes < 3 and zscore_session_avg_playback < 3 and zscore_session_avg_settings < 3 and zscore_session_avg_songs < 3 and zscore_session_avg_time_away < 3 and zscore_time_window < 3'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:45.627455Z",
     "start_time": "2020-05-07T23:52:42.187941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22278"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:49.441509Z",
     "start_time": "2020-05-07T23:52:49.233761Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users = df_users.filter(_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:52:54.241801Z",
     "start_time": "2020-05-07T23:52:51.660591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------\n",
      " canceled                          | 0                    \n",
      " male                              | 1                    \n",
      " paid                              | 1                    \n",
      " avg_daily_actions                 | 56.833333333333336   \n",
      " avg_session_duration              | 8.064825676115998    \n",
      " n_actions                         | 682                  \n",
      " n_added_to_playlist               | 20                   \n",
      " n_ads                             | 3                    \n",
      " n_days                            | 12                   \n",
      " n_dislikes                        | 10                   \n",
      " n_downgrades                      | 1                    \n",
      " n_errors                          | 3                    \n",
      " n_friends                         | 12                   \n",
      " n_help                            | 5                    \n",
      " n_home                            | 27                   \n",
      " n_likes                           | 25                   \n",
      " n_sess                            | 9                    \n",
      " n_settings                        | 4                    \n",
      " n_songs                           | 557                  \n",
      " n_upgrades                        | 0                    \n",
      " session_avg_actions               | 17.377708646219023   \n",
      " session_avg_added_playlist        | 0.5096102242292969   \n",
      " session_avg_ads                   | 0.07644153363439453  \n",
      " session_avg_dislikes              | 0.25480511211464846  \n",
      " session_avg_errors                | 0.07644153363439453  \n",
      " session_avg_friends               | 0.3057661345375781   \n",
      " session_avg_help                  | 0.12740255605732423  \n",
      " session_avg_home                  | 0.6879738027095508   \n",
      " session_avg_likes                 | 0.6370127802866211   \n",
      " session_avg_playback              | 1.0                  \n",
      " session_avg_settings              | 0.10192204484585937  \n",
      " session_avg_songs                 | 14.192644744785918   \n",
      " session_avg_time_away             | 152.10534722222224   \n",
      " time_window                       | 1255.4402777777777   \n",
      " zscore_avg_daily_actions          | 0.20603868603291917  \n",
      " zscore_avg_session_duration       | 0.11185436365031809  \n",
      " zscore_session_avg_actions        | 0.18596546423748517  \n",
      " zscore_session_avg_added_playlist | 0.5884073750750026   \n",
      " zscore_session_avg_ads            | 0.623805908439051    \n",
      " zscore_session_avg_dislikes       | 0.3420406240815475   \n",
      " zscore_session_avg_errors         | 1.1137021633840984   \n",
      " zscore_session_avg_friends        | 0.10565088459939553  \n",
      " zscore_session_avg_help           | 0.3161425885687867   \n",
      " zscore_session_avg_home           | 0.12372297295925637  \n",
      " zscore_session_avg_likes          | 0.37326010732244697  \n",
      " zscore_session_avg_playback       | 0.02763453275136613  \n",
      " zscore_session_avg_settings       | 0.052702451110523685 \n",
      " zscore_session_avg_songs          | 0.08738572096842102  \n",
      " zscore_session_avg_time_away      | 0.09843974859677213  \n",
      " zscore_time_window                | 0.6008273431313189   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.show(1, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:53:01.250863Z",
     "start_time": "2020-05-07T23:53:00.972726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double, zscore_avg_daily_actions: double, zscore_avg_session_duration: double, zscore_session_avg_actions: double, zscore_session_avg_added_playlist: double, zscore_session_avg_ads: double, zscore_session_avg_dislikes: double, zscore_session_avg_errors: double, zscore_session_avg_friends: double, zscore_session_avg_help: double, zscore_session_avg_home: double, zscore_session_avg_likes: double, zscore_session_avg_playback: double, zscore_session_avg_settings: double, zscore_session_avg_songs: double, zscore_session_avg_time_away: double, zscore_time_window: double]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:53:05.394401Z",
     "start_time": "2020-05-07T23:53:04.048600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20134"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:53:26.017983Z",
     "start_time": "2020-05-07T23:53:26.015739Z"
    }
   },
   "outputs": [],
   "source": [
    "# _df_filtered = df[(np.abs(stats.zscore(df[_columns_to_check_outliers])) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T18:00:04.233835Z",
     "start_time": "2020-05-02T18:00:03.769010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " n_dislikes                 | 5                    \n",
      " session_avg_settings       | 0.09123393902531743  \n",
      " n_help                     | 1                    \n",
      " userId                     | 200002               \n",
      " n_actions                  | 395                  \n",
      " session_avg_help           | 0.045616969512658714 \n",
      " n_settings                 | 2                    \n",
      " session_avg_playback       | 0.997184759370486    \n",
      " n_sess                     | 5                    \n",
      " avg_session_duration       | 5.75821870604782     \n",
      " session_avg_home           | 1.0491902987911503   \n",
      " n_downgrades               | 0                    \n",
      " canceled                   | 1                    \n",
      " session_avg_likes          | 0.6842545426898806   \n",
      " n_likes                    | 15                   \n",
      " session_avg_dislikes       | 0.22808484756329356  \n",
      " n_songs                    | 310                  \n",
      " n_upgrades                 | 1                    \n",
      " n_ads                      | 11                   \n",
      " n_days                     | 6                    \n",
      " time_window                | 689.8877777777777    \n",
      " n_added_to_playlist        | 6                    \n",
      " session_avg_friends        | 0.09123393902531743  \n",
      " n_friends                  | 2                    \n",
      " avg_daily_actions          | 65.83333333333333    \n",
      " paid                       | 0                    \n",
      " session_avg_time_away      | 166.99152777777778   \n",
      " session_avg_ads            | 0.5017866646392458   \n",
      " session_avg_errors         | 0.0                  \n",
      " session_avg_actions        | 18.018702957500192   \n",
      " n_home                     | 23                   \n",
      " session_avg_added_playlist | 0.27370181707595226  \n",
      " n_errors                   | 0                    \n",
      " male                       | 1                    \n",
      " session_avg_songs          | 14.1412605489242     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.where(df_users.userId == int(200002)).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T17:30:06.552478Z",
     "start_time": "2020-05-02T17:28:15.720923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " canceled                   | 0    \n",
      " male                       | 1    \n",
      " paid                       | 1    \n",
      " avg_daily_actions          | 57   \n",
      " avg_session_duration       | 8    \n",
      " n_actions                  | 682  \n",
      " n_added_to_playlist        | 20   \n",
      " n_ads                      | 3    \n",
      " n_days                     | 12   \n",
      " n_dislikes                 | 10   \n",
      " n_downgrades               | 1    \n",
      " n_errors                   | 3    \n",
      " n_friends                  | 12   \n",
      " n_help                     | 5    \n",
      " n_home                     | 27   \n",
      " n_likes                    | 25   \n",
      " n_sess                     | 9    \n",
      " n_settings                 | 4    \n",
      " n_songs                    | 557  \n",
      " n_upgrades                 | 0    \n",
      " session_avg_actions        | 18   \n",
      " session_avg_added_playlist | 1    \n",
      " session_avg_ads            | 0    \n",
      " session_avg_dislikes       | 0    \n",
      " session_avg_errors         | 0    \n",
      " session_avg_friends        | 0    \n",
      " session_avg_help           | 0    \n",
      " session_avg_home           | 1    \n",
      " session_avg_likes          | 1    \n",
      " session_avg_playback       | 1    \n",
      " session_avg_settings       | 0    \n",
      " session_avg_songs          | 14   \n",
      " session_avg_time_away      | 152  \n",
      " time_window                | 1255 \n",
      "-RECORD 1--------------------------\n",
      " canceled                   | 1    \n",
      " male                       | 0    \n",
      " paid                       | 1    \n",
      " avg_daily_actions          | 58   \n",
      " avg_session_duration       | 4    \n",
      " n_actions                  | 349  \n",
      " n_added_to_playlist        | 12   \n",
      " n_ads                      | 2    \n",
      " n_days                     | 6    \n",
      " n_dislikes                 | 2    \n",
      " n_downgrades               | 0    \n",
      " n_errors                   | 1    \n",
      " n_friends                  | 7    \n",
      " n_help                     | 1    \n",
      " n_home                     | 14   \n",
      " n_likes                    | 10   \n",
      " n_sess                     | 7    \n",
      " n_settings                 | 1    \n",
      " n_songs                    | 287  \n",
      " n_upgrades                 | 1    \n",
      " session_avg_actions        | 17   \n",
      " session_avg_added_playlist | 1    \n",
      " session_avg_ads            | 0    \n",
      " session_avg_dislikes       | 0    \n",
      " session_avg_errors         | 0    \n",
      " session_avg_friends        | 0    \n",
      " session_avg_help           | 0    \n",
      " session_avg_home           | 1    \n",
      " session_avg_likes          | 0    \n",
      " session_avg_playback       | 1    \n",
      " session_avg_settings       | 0    \n",
      " session_avg_songs          | 14   \n",
      " session_avg_time_away      | 65   \n",
      " time_window                | 410  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WARN: Only round to display\n",
    "# Enforces the order for some columns\n",
    "df_users.select([sround(c, 0).cast(dataType = IntegerType()).alias(c) for c in columns_all]).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Save the final dataset to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:53:40.299906Z",
     "start_time": "2020-05-07T23:53:38.715602Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.select(columns_all).fillna(0).toPandas().to_csv('sparkify_data_full_dataset_final_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:53.790091Z",
     "start_time": "2020-03-21T20:49:53.293466Z"
    }
   },
   "outputs": [],
   "source": [
    "df.agg(countDistinct(df.userId).alias('unique_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).groupBy('canceled').agg(count(df_users.canceled).alias('total')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advertises number (per session and all)\n",
    "    - The user **100010** returned after some idle time and received a considerable amount of advertises;\n",
    "    - Also, after thumbs down, I received two advertisements on four sounds. Then canceled the service.\n",
    "- Number of sessions\n",
    "- Paid subscription time\n",
    "- Avg songs before an ad\n",
    "- Number of skipped songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_date(df.ts.cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.userId == user_id).select(['artist',\n",
    " 'auth',\n",
    " 'firstName',\n",
    " 'gender',\n",
    " 'itemInSession',\n",
    " 'lastName',\n",
    " 'length',\n",
    " 'level', \n",
    " 'page',\n",
    " 'sessionId',\n",
    " 'song', \n",
    " 'ts', \n",
    " 'userId']).orderBy('sessionId', 'itemInSession').withColumn('datetime', date_format((df.ts/1000).cast(dataType=TimestampType()), 'HH:mm:ss dd-MM-YYYY')).show(350, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:54:43.591418Z",
     "start_time": "2020-05-07T23:54:43.583683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n",
      "\n",
      "Columns to train: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n"
     ]
    }
   ],
   "source": [
    "columns_to_exclude = set(['userId'] + zscore_columns)\n",
    "\n",
    "columns_to_use = list(set(df_users.columns) - columns_to_exclude)\n",
    "\n",
    "columns_to_train = list(set(columns_to_use) - set(['canc']))\n",
    "\n",
    "columns_to_use.sort()\n",
    "columns_to_train.sort()\n",
    "\n",
    "print(f'Columns: {columns_to_use}\\n')\n",
    "print(f'Columns to train: {columns_to_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:54:46.798983Z",
     "start_time": "2020-05-07T23:54:46.794066Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_LABEL = 'canceled'\n",
    "TRAIN_SPLIT_RATIO = .8\n",
    "TEST_SPLIT_RATIO = .2\n",
    "\n",
    "SPLIT_RATIO = [TRAIN_SPLIT_RATIO, TEST_SPLIT_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T23:54:52.381486Z",
     "start_time": "2020-05-07T23:54:52.356859Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_predictions):\n",
    "    \n",
    "    # auc = roc_auc_score(y_test, y_predictions)\n",
    "    cm = confusion_matrix(y_test, y_predictions, labels = [1, 0])\n",
    "    \n",
    "    tn = cm[1, 1]\n",
    "    tp = cm[0, 0]\n",
    "    fp = cm[1, 0]\n",
    "    fn = cm[0, 1]\n",
    "    \n",
    "    total = np.sum(cm) # tn + tp + fn + fp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn) \n",
    "    \n",
    "    print(cm)\n",
    "\n",
    "def evaluate_multiclass_classifier(predictions, columns):\n",
    "    metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall', 'recall']\n",
    "    \n",
    "    result = {}\n",
    "    for metric in metrics_to_evaluate:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol = columns[0], predictionCol = columns[1], metricName = metric)\n",
    "        value = evaluator.evaluate(predictions)\n",
    "        result[metric] = value\n",
    "        print(f'{metric}: {value}') \n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_random_forest_classifier(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    # f_scaler = MaxAbsScaler(inputCol=\"num_features\", outputCol=\"num_features_escaled\")\n",
    "    # f_scaler = Normalizer(inputCol = \"num_features\", outputCol = \"num_features_escaled\", p = 3)\n",
    "    # f_scaler = MinMaxScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', )\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    # rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10, maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42)\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', seed = 42)\n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, rf_classifier, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def train_logistic_regression(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'idx_labels', maxIter = 1000, regParam = 0, elasticNetParam = 0, family = 'binomial', aggregationDepth = 15) \n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, lr, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "    \n",
    "\n",
    "def create_pipeline(model):\n",
    "    \n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, model ])\n",
    "    return pipeline\n",
    "\n",
    "def create_random_forest_pipeline():\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'canceled', featuresCol = 'features', seed = 42)\n",
    "    return create_pipeline(rf_classifier)\n",
    "\n",
    "def create_gradient_boost_pipeline():\n",
    "    gbt_classifier = GBTClassifier(labelCol = 'canceled', maxDepth = 5, maxIter = 100, seed = 42)\n",
    "    return create_pipeline(gbt_classifier)\n",
    "\n",
    "def create_logistic_regression_pipeline():\n",
    "    lr_classifier = LogisticRegression(featuresCol = 'features', labelCol = 'idx_labels', weightCol = 'class_weights') \n",
    "    return create_pipeline(lr_classifier)\n",
    "\n",
    "def create_decision_tree_pipeline():\n",
    "    dt_classifier = DecisionTreeClassifier(labelCol = 'canceled', featuresCol = 'features', seed = 42)\n",
    "    return create_pipeline(dt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:47.811584Z",
     "start_time": "2020-05-03T22:06:32.217201Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_logistic_regression(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:48.039665Z",
     "start_time": "2020-05-03T22:06:47.814507Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:48.049295Z",
     "start_time": "2020-05-03T22:06:48.041153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 537  394]\n",
      " [ 179 3036]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:39:28.738718Z",
     "start_time": "2020-05-03T19:39:28.722464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 912  690]\n",
      " [ 313 5305]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:33:43.529567Z",
     "start_time": "2020-05-03T19:33:43.514550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 875  727]\n",
      " [ 345 5273]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:03.408203Z",
     "start_time": "2020-05-03T19:22:52.520852Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:04.899188Z",
     "start_time": "2020-05-03T19:23:03.409659Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:06.416171Z",
     "start_time": "2020-05-03T19:23:06.400371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 824  778]\n",
      " [ 116 5502]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:16:46.273162Z",
     "start_time": "2020-05-03T19:16:46.260335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 737  666]\n",
      " [ 123 4712]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:09:12.152496Z",
     "start_time": "2020-05-03T19:09:12.137997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 734  669]\n",
      " [ 133 4702]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T23:32:41.611423Z",
     "start_time": "2020-04-04T23:32:31.035584Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:46:31.224705Z",
     "start_time": "2020-05-02T23:46:31.215982Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:47:12.105081Z",
     "start_time": "2020-05-02T23:47:11.431773Z"
    }
   },
   "outputs": [],
   "source": [
    "df_r = predictions.select('canceled', 'prediction').toPandas()\n",
    "df_r['prediction'] = df_r.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:38:41.863345Z",
     "start_time": "2020-05-08T00:38:41.844118Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "\n",
    "class Recall(Evaluator):\n",
    "    \n",
    "    def __init__(self, predictionCol = 'prediction', labelCol = 'label'):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.uid = str(uuid.uuid4())\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        \n",
    "        tp = dataset.where((dataset[self.labelCol] == 1) & (dataset[self.predictionCol] == 1)).count()\n",
    "        fp = dataset.where((dataset[self.labelCol] == 0) & (dataset[self.predictionCol] == 1)).count()\n",
    "        tn = dataset.where((dataset[self.labelCol] == 0) & (dataset[self.predictionCol] == 0)).count()\n",
    "        fn = dataset.where((dataset[self.labelCol] == 1) & (dataset[self.predictionCol] == 0)).count()\n",
    "        \n",
    "        # fnr = fn / (1 if (tp + fn) == 0 else (tp + fn))\n",
    "        \n",
    "        return (100 / (tp + fn )) * tp\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "    \n",
    "from sklearn.metrics import f1_score, fbeta_score, make_scorer, roc_auc_score\n",
    "\n",
    "# beta - The recall weight \n",
    "beta_scorer = make_scorer(fbeta_score, average='binary', beta = 2)\n",
    "\n",
    "def get_beta_score(y_test, y_predict):\n",
    "    return fbeta_score(y_test, y_predict, average = 'binary', labels = [1, 0], beta = 2)\n",
    "    \n",
    "    \n",
    "def plot_metrics(y_test, y_predictions):\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_predictions)\n",
    "    cm = confusion_matrix(y_test, y_predictions, labels = [1, 0])\n",
    "    \n",
    "    tn = cm[1, 1]\n",
    "    tp = cm[0, 0]\n",
    "    fp = cm[1, 0]\n",
    "    fn = cm[0, 1]\n",
    "    \n",
    "    total = np.sum(cm) # tn + tp + fn + fp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn) \n",
    "\n",
    "    beta = get_beta_score(y_test, y_predictions)\n",
    "    \n",
    "    print(cm)\n",
    "    print('')\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('auc......................%7.4f' % auc)\n",
    "    print('beta.....................%7.4f' % beta)\n",
    "    print('F1 macro.................%7.4f' % f1_score(y_test, y_predictions, average = 'macro'))\n",
    "    print('F1 micro.................%7.4f' % f1_score(y_test, y_predictions, average = 'micro'))\n",
    "    print('F1 weighted..............%7.4f' % f1_score(y_test, y_predictions, average = 'weighted'))\n",
    "    # f1 = (2 * tp ) / (2 * tp + fp + fn) - f1_score with average 'binary'\n",
    "    print('F1 binary................%7.4f' % f1_score(y_test, y_predictions, average = 'binary', labels = [1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:07.382355Z",
     "start_time": "2020-05-08T00:07:07.332258Z"
    }
   },
   "outputs": [],
   "source": [
    "_eval = BinaryClassificationEvaluator(rawPredictionCol = 'prediction', labelCol = 'canceled', metricName = 'areaUnderROC') # Recall(labelCol = 'canceled')\n",
    "\n",
    "def create_grid_search(pipeline, param_grid):\n",
    "    \n",
    "    return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = _eval, numFolds = 5, parallelism = 16, seed = 42)\n",
    "    # return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = _eval, numFolds = 3, parallelism = 16, seed = 42)\n",
    "\n",
    "def random_forest_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_rf = ParamGridBuilder().addGrid(model.maxDepth, [5, 10, 15, 20, 25]) \n",
    "    grid_rf = grid_rf.addGrid(model.impurity, ['gini']) \n",
    "    grid_rf = grid_rf.addGrid(model.maxBins, [5, 10, 15, 20, 25, 30, 35, 40])\n",
    "    grid_rf = grid_rf.addGrid(model.numTrees, [10, 20, 40, 60, 70])\n",
    "    grid_rf = grid_rf.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_rf)}')\n",
    "        \n",
    "    return create_grid_search(pipeline, grid_rf)\n",
    "\n",
    "def gradient_boost_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_gbt = ParamGridBuilder().addGrid(model.maxDepth, [2, 4, 6, 8, 10])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxIter, [20, 25, 40, 50, 100])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxBins, [2, 32])\n",
    "    grid_gbt = grid_gbt.addGrid(model.subsamplingRate, [.5, .8, 1])\n",
    "    grid_gbt = grid_gbt.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_gbt)}')\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_gbt)\n",
    "\n",
    "def logistic_regression_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_lr = ParamGridBuilder().addGrid(model.aggregationDepth, [2, 5, 10])\n",
    "    grid_lr = grid_lr.addGrid(model.elasticNetParam, [.0, .5, 1.0])\n",
    "    grid_lr = grid_lr.addGrid(model.fitIntercept, [ True, False ])\n",
    "    grid_lr = grid_lr.addGrid(model.standardization, [ True, False ])\n",
    "    grid_lr = grid_lr.addGrid(model.maxIter, [10, 100, 1000])\n",
    "    grid_lr = grid_lr.addGrid(model.regParam, [.0, .01, .5, 2.0])\n",
    "    grid_lr = grid_lr.addGrid(model.tol, [.0001])\n",
    "    grid_lr = grid_lr.addGrid(model.weightCol, [ 'class_weights' ])\n",
    "\n",
    "    grid_lr = grid_lr.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_lr)}')\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_lr)\n",
    "\n",
    "def decision_tree_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_dt = ParamGridBuilder()\n",
    "    grid_dt = grid_dt.addGrid(model.maxDepth, [5, 10, 15, 25, 30]) \n",
    "    grid_dt = grid_dt.addGrid(model.impurity, ['gini', 'entropy']) \n",
    "    grid_dt = grid_dt.addGrid(model.maxBins, [2, 8, 16, 32])\n",
    "    grid_dt = grid_dt.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_dt)}')\n",
    "        \n",
    "    return create_grid_search(pipeline, grid_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:10.145588Z",
     "start_time": "2020-05-08T00:07:10.140670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:13.175629Z",
     "start_time": "2020-05-08T00:07:13.148015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "# data = df_users.select(columns_to_use).fillna(0)\n",
    "\n",
    "# Split train/test\n",
    "(train_df, test_df) = df_users.randomSplit(SPLIT_RATIO, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:13.975429Z",
     "start_time": "2020-05-08T00:07:13.921994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double, zscore_avg_daily_actions: double, zscore_avg_session_duration: double, zscore_session_avg_actions: double, zscore_session_avg_added_playlist: double, zscore_session_avg_ads: double, zscore_session_avg_dislikes: double, zscore_session_avg_errors: double, zscore_session_avg_friends: double, zscore_session_avg_help: double, zscore_session_avg_home: double, zscore_session_avg_likes: double, zscore_session_avg_playback: double, zscore_session_avg_settings: double, zscore_session_avg_songs: double, zscore_session_avg_time_away: double, zscore_time_window: double]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:16.509185Z",
     "start_time": "2020-05-08T00:07:15.602201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20134"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() + test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:22.274207Z",
     "start_time": "2020-05-08T00:07:21.907857Z"
    }
   },
   "outputs": [],
   "source": [
    "balancing_ratio = train_df.filter(train_df.canceled == 0).count() / train_df.count()\n",
    "\n",
    "train_df = train_df.withColumn('class_weights', when(train_df.canceled == 1, balancing_ratio).otherwise(1 - balancing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:07:25.515069Z",
     "start_time": "2020-05-08T00:07:25.308112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------\n",
      " canceled                          | 0                   \n",
      " male                              | 0                   \n",
      " paid                              | 0                   \n",
      " avg_daily_actions                 | 2.0                 \n",
      " avg_session_duration              | 0.0636111111111111  \n",
      " n_actions                         | 2                   \n",
      " n_added_to_playlist               | 0                   \n",
      " n_ads                             | 0                   \n",
      " n_days                            | 1                   \n",
      " n_dislikes                        | 0                   \n",
      " n_downgrades                      | 0                   \n",
      " n_errors                          | 0                   \n",
      " n_friends                         | 0                   \n",
      " n_help                            | 0                   \n",
      " n_home                            | 0                   \n",
      " n_likes                           | 0                   \n",
      " n_sess                            | 1                   \n",
      " n_settings                        | 0                   \n",
      " n_songs                           | 2                   \n",
      " n_upgrades                        | 0                   \n",
      " session_avg_actions               | 16.468039241141774  \n",
      " session_avg_added_playlist        | 0.0                 \n",
      " session_avg_ads                   | 0.0                 \n",
      " session_avg_dislikes              | 0.0                 \n",
      " session_avg_errors                | 0.0                 \n",
      " session_avg_friends               | 0.0                 \n",
      " session_avg_help                  | 0.0                 \n",
      " session_avg_home                  | 0.0                 \n",
      " session_avg_likes                 | 0.0                 \n",
      " session_avg_playback              | 1.0                 \n",
      " session_avg_settings              | 0.0                 \n",
      " session_avg_songs                 | 16.468039241141774  \n",
      " session_avg_time_away             | 0.0                 \n",
      " time_window                       | 0.0636111111111111  \n",
      " zscore_avg_daily_actions          | 0.8030323572575322  \n",
      " zscore_avg_session_duration       | 1.4302449976338985  \n",
      " zscore_session_avg_actions        | 0.5368998289684908  \n",
      " zscore_session_avg_added_playlist | 2.0977952596293057  \n",
      " zscore_session_avg_ads            | 0.7170554246488922  \n",
      " zscore_session_avg_dislikes       | 0.8790997818976518  \n",
      " zscore_session_avg_errors         | 0.3523234241700469  \n",
      " zscore_session_avg_friends        | 0.8262395645773986  \n",
      " zscore_session_avg_help           | 0.7837030836691393  \n",
      " zscore_session_avg_home           | 1.224477101697834   \n",
      " zscore_session_avg_likes          | 2.215075857996365   \n",
      " zscore_session_avg_playback       | 0.02763453275136613 \n",
      " zscore_session_avg_settings       | 0.5337025307206957  \n",
      " zscore_session_avg_songs          | 2.1498123577987625  \n",
      " zscore_session_avg_time_away      | 0.9382256285773164  \n",
      " zscore_time_window                | 2.1690342764817157  \n",
      " class_weights                     | 0.22471492315319785 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:26:35.215032Z",
     "start_time": "2020-05-08T00:07:57.612024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 432\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_logistic_regression_pipeline()\n",
    "lr = logistic_regression_grid_search(pipeline)\n",
    "rf_results = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:41:23.745686Z",
     "start_time": "2020-05-08T00:41:23.504873Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:41:26.124696Z",
     "start_time": "2020-05-08T00:41:25.897096Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:41:32.517900Z",
     "start_time": "2020-05-08T00:41:32.496459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 704  172]\n",
      " [ 411 2711]]\n",
      "\n",
      "accuracy................. 0.8542\n",
      "precision................ 0.6314\n",
      "recall................... 0.8037\n",
      "auc...................... 0.8360\n",
      "beta..................... 0.7621\n",
      "F1 macro................. 0.8050\n",
      "F1 micro................. 0.8542\n",
      "F1 weighted.............. 0.8600\n",
      "F1 binary................ 0.7072\n"
     ]
    }
   ],
   "source": [
    "plot_metrics(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:27:58.644221Z",
     "start_time": "2020-05-08T00:27:58.639087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8387436023739107"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rf_results.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:38:46.596580Z",
     "start_time": "2020-05-03T22:20:25.468519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 432\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_logistic_regression_pipeline()\n",
    "lr = logistic_regression_grid_search(pipeline)\n",
    "rf_results = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:22.701524Z",
     "start_time": "2020-05-03T22:39:22.440311Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:25.013398Z",
     "start_time": "2020-05-03T22:39:24.789706Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:29.868076Z",
     "start_time": "2020-05-03T22:39:29.856256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 723  208]\n",
      " [ 443 2772]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:41:06.460073Z",
     "start_time": "2020-05-03T22:41:06.454447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321030434677803"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rf_results.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:41:13.309427Z",
     "start_time": "2020-05-03T22:41:13.303652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5,\n",
       " {Param(parent='LogisticRegression_ef68b4aed01a', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 10,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='fitIntercept', doc='whether to fit an intercept term.'): False,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='maxIter', doc='max number of iterations (>= 0).'): 1000,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='regParam', doc='regularization parameter (>= 0).'): 2.0,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 0.0001,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.'): 'class_weights'})"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(rf_results.avgMetrics, rf_results.getEstimatorParamMaps())))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:53:12.413935Z",
     "start_time": "2020-05-03T20:53:12.407383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_1f4e8dd33901', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='fitIntercept', doc='whether to fit an intercept term'): False,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='labelCol', doc='label column name'): 'idx_labels',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='maxIter', doc='maximum number of iterations (>= 0)'): 1000,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='regParam', doc='regularization parameter (>= 0)'): 0.0,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:43:05.405761Z",
     "start_time": "2020-05-08T00:43:05.394842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8264047243402131,\n",
       " 0.826036570456598,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8264047243402131,\n",
       " 0.8258444921516166,\n",
       " 0.8154961063636262,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8226357065638533,\n",
       " 0.8250672613385864,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8226357065638533,\n",
       " 0.8248011376789177,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8264047243402131,\n",
       " 0.8263226733699227,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8264090848012862,\n",
       " 0.7614979384786977,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8255446601541605,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8256782142941993,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8239358036121781,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8238163275552406,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8244031917491826,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8258045586168941,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.826036570456598,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8264047243402131,\n",
       " 0.8258444921516166,\n",
       " 0.8154961063636262,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8226357065638533,\n",
       " 0.8250672613385864,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8226357065638533,\n",
       " 0.8248011376789177,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8264047243402131,\n",
       " 0.8263226733699227,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8264090848012862,\n",
       " 0.7614979384786977,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8255446601541605,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8256782142941993,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8239358036121781,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8238163275552406,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8244031917491826,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8258045586168941,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.826036570456598,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8387436023739107,\n",
       " 0.82771067345102,\n",
       " 0.8174141646911088,\n",
       " 0.8042672831257991,\n",
       " 0.8264047243402131,\n",
       " 0.8258444921516166,\n",
       " 0.8154961063636262,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8387436023739107,\n",
       " 0.8271202943522074,\n",
       " 0.8154556859917589,\n",
       " 0.8033303552105432,\n",
       " 0.8226357065638533,\n",
       " 0.8250672613385864,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8322642080196991,\n",
       " 0.8267043498728722,\n",
       " 0.8176666452025272,\n",
       " 0.804337628218464,\n",
       " 0.8226357065638533,\n",
       " 0.8248011376789177,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8322642080196991,\n",
       " 0.825802408850783,\n",
       " 0.8174373121789178,\n",
       " 0.8055684463938599,\n",
       " 0.8264047243402131,\n",
       " 0.8263226733699227,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8260995251578429,\n",
       " 0.7582965196006981,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8264090848012862,\n",
       " 0.7614979384786977,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8265541540097353,\n",
       " 0.7602219694062041,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8255446601541605,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.826376693782942,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8256782142941993,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8260103398399916,\n",
       " 0.7660916194155776,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8239358036121781,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.825239656492356,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8264047243402131,\n",
       " 0.8238163275552406,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8387436023739107,\n",
       " 0.8246947002517446,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8244031917491826,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8249166712310957,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8226357065638533,\n",
       " 0.8258045586168941,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.8322642080196991,\n",
       " 0.8254178230932083,\n",
       " 0.5,\n",
       " 0.5]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:31:03.001821Z",
     "start_time": "2020-05-08T00:31:02.953369Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 40\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_decision_tree_pipeline()\n",
    "dt = decision_tree_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:33:26.658919Z",
     "start_time": "2020-05-08T00:31:19.313271Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_results = dt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:36:21.180156Z",
     "start_time": "2020-05-08T00:36:20.726570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 603  273]\n",
      " [ 181 2941]]\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_results.bestModel.transform(test_df)\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:39:01.861787Z",
     "start_time": "2020-05-08T00:39:01.827553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 603  273]\n",
      " [ 181 2941]]\n",
      "\n",
      "accuracy................. 0.8864\n",
      "precision................ 0.7691\n",
      "recall................... 0.6884\n",
      "auc...................... 0.8152\n",
      "beta..................... 0.7031\n",
      "F1 macro................. 0.8274\n",
      "F1 micro................. 0.8864\n",
      "F1 weighted.............. 0.8841\n",
      "F1 binary................ 0.7265\n"
     ]
    }
   ],
   "source": [
    "plot_metrics(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:16:30.207063Z",
     "start_time": "2020-05-03T23:16:30.185989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 40\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_decision_tree_pipeline()\n",
    "dt = decision_tree_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:18:33.572521Z",
     "start_time": "2020-05-03T23:16:31.190216Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_results = dt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:18:43.072995Z",
     "start_time": "2020-05-03T23:18:42.609695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 616  315]\n",
      " [ 147 3068]]\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_results.bestModel.transform(test_df)\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:06:25.137946Z",
     "start_time": "2020-05-03T23:06:24.671232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 603  328]\n",
      " [ 156 3059]]\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_results.bestModel.transform(test_df)\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T23:19:00.059217Z",
     "start_time": "2020-05-03T23:19:00.051624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7735970806669392,\n",
       " {Param(parent='DecisionTreeClassifier_c3ce51f2b4c5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 30,\n",
       "  Param(parent='DecisionTreeClassifier_c3ce51f2b4c5', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       "  Param(parent='DecisionTreeClassifier_c3ce51f2b4c5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32})"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(dt_results.avgMetrics, dt_results.getEstimatorParamMaps())))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T00:45:21.035500Z",
     "start_time": "2020-05-08T00:45:20.994606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 200\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T01:31:58.162721Z",
     "start_time": "2020-05-08T00:45:30.507097Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T01:40:05.574229Z",
     "start_time": "2020-05-08T01:40:05.272881Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T01:40:12.857129Z",
     "start_time": "2020-05-08T01:40:11.592713Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T01:41:37.420213Z",
     "start_time": "2020-05-08T01:41:37.395040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 594  282]\n",
      " [ 103 3019]]\n",
      "\n",
      "accuracy................. 0.9037\n",
      "precision................ 0.8522\n",
      "recall................... 0.6781\n",
      "auc...................... 0.8225\n",
      "beta..................... 0.7070\n",
      "F1 macro................. 0.8477\n",
      "F1 micro................. 0.9037\n",
      "F1 weighted.............. 0.8996\n",
      "F1 binary................ 0.7552\n"
     ]
    }
   ],
   "source": [
    "plot_metrics(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:00:38.935148Z",
     "start_time": "2020-05-03T01:00:38.909068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 200\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:28:55.722489Z",
     "start_time": "2020-05-03T01:00:41.781911Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:43.033732Z",
     "start_time": "2020-05-03T01:44:42.513443Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:45.168583Z",
     "start_time": "2020-05-03T01:44:43.933933Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:54.443560Z",
     "start_time": "2020-05-03T01:44:54.431890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 605  326]\n",
      " [ 120 3095]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T14:20:10.954391Z",
     "start_time": "2020-04-05T14:20:10.948573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64.35829817256004,\n",
       " {Param(parent='RandomForestClassifier_5b6344e9b3f5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 25,\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='numTrees', doc='Number of trees to train (>= 1).'): 70})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(cv_rf_results.avgMetrics, cv_rf_results.getEstimatorParamMaps())))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T05:00:18.612314Z",
     "start_time": "2020-05-08T01:42:39.875346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 150\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:22:37.673117Z",
     "start_time": "2020-05-08T11:22:37.011254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 631  245]\n",
      " [ 138 2984]]\n",
      "\n",
      "accuracy................. 0.9042\n",
      "precision................ 0.8205\n",
      "recall................... 0.7203\n",
      "auc...................... 0.8381\n",
      "beta..................... 0.7384\n",
      "F1 macro................. 0.8534\n",
      "F1 micro................. 0.9042\n",
      "F1 weighted.............. 0.9019\n",
      "F1 binary................ 0.7672\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_metrics(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T02:47:18.671351Z",
     "start_time": "2020-05-03T23:25:12.025659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 150\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T02:59:28.724112Z",
     "start_time": "2020-05-04T02:59:28.113458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 640  291]\n",
      " [ 164 3051]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T03:01:08.094799Z",
     "start_time": "2020-05-04T03:01:08.089761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8262411338076534"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(_results.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T14:16:38.672206Z",
     "start_time": "2020-03-25T02:53:27.924737Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T02:53:10.319833Z",
     "start_time": "2020-05-03T02:53:09.685054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 637  294]\n",
      " [ 274 2941]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T02:01:07.081948Z",
     "start_time": "2020-03-23T02:01:07.074972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_ba2654d5e488', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'all',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='labelCol', doc='label column name'): 'canceled',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.'): 32,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxDepth', doc='Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxIter', doc='maximum number of iterations (>= 0)'): 20,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.'): 1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='seed', doc='random seed'): -6852653717611865011,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimator params and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:56.517560Z",
     "start_time": "2020-03-01T14:06:56.488834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = cv_gbt_results.avgMetrics\n",
    "params = [{p.name: v for p, v in m.items()} for m in cv_gbt.getEstimatorParamMaps()]\n",
    "params_pd = pd.DataFrame(params)\n",
    "params_pd['score'] = scores\n",
    "params_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
