{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:36.697877Z",
     "start_time": "2020-03-21T20:47:29.265702Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import min as smin, max as smax, sum as ssum, round as sround\n",
    "from pyspark.sql.functions import isnan, isnull, when, first, avg, last, count, countDistinct, col, lag, lead, coalesce, lit, split, trim\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_date, date_format, from_unixtime, to_timestamp\n",
    "\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    " \n",
    "import jupyter_utils as j\n",
    "\n",
    "from pyspark import SparkContext\n",
    " \n",
    "SparkContext.setSystemProperty('spark.logConf', 'True')\n",
    "SparkContext.setSystemProperty('spark.default.parallelism', '16')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '6g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '4g')\n",
    "SparkContext.setSystemProperty('spark.reducer.maxSizeInFlight', '96m')\n",
    "SparkContext.setSystemProperty('spark.shuffle.consolidateFiles', 'True') \n",
    "SparkContext.setSystemProperty('spark.shuffle.service.index.cache.size', '500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:04:04.110826Z",
     "start_time": "2020-03-14T21:04:04.104465Z"
    }
   },
   "outputs": [],
   "source": [
    "j.reload(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:36.702198Z",
     "start_time": "2020-03-21T20:47:36.699839Z"
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'sparkify_full_csv_data.csv'\n",
    "filepath = 'medium_sparkify_event_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:38.452278Z",
     "start_time": "2020-03-21T20:47:36.703870Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:42.151895Z",
     "start_time": "2020-03-21T20:47:42.125296Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:50.469111Z",
     "start_time": "2020-03-21T20:47:46.744283Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").csv(filepath)\n",
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:50.525898Z",
     "start_time": "2020-03-21T20:47:50.470614Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "print(df.storageLevel)\n",
    "\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print(df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:47:53.568936Z",
     "start_time": "2020-03-21T20:47:53.551739Z"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:44:23.082123Z",
     "start_time": "2020-03-14T21:44:22.438015Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T21:46:58.898921Z",
     "start_time": "2020-03-14T21:46:58.481407Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('auth').agg(count(col('auth'))).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T22:13:17.244523Z",
     "start_time": "2020-03-14T22:13:17.152911Z"
    }
   },
   "outputs": [],
   "source": [
    "df.where(~df.auth.isin(['Logged In', 'Cancelled'])).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T22:03:26.627411Z",
     "start_time": "2020-03-14T22:03:26.549337Z"
    }
   },
   "outputs": [],
   "source": [
    "df.where((df.auth == 'Logged In') & (df.page == 'Home')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:02.213188Z",
     "start_time": "2020-03-21T20:48:02.199435Z"
    }
   },
   "outputs": [],
   "source": [
    "log4jLogger = spark.sparkContext._jvm.org.apache.log4j\n",
    "\n",
    "LOGGER = log4jLogger.LogManager.getLogger('driver_logger')\n",
    "\n",
    "def info(message, print_on_notebook = True):\n",
    "    LOGGER.info(message)\n",
    "    \n",
    "    if print_on_notebook:\n",
    "        print(message)\n",
    "    \n",
    "info('Logger instance created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:05.668990Z",
     "start_time": "2020-03-21T20:48:05.652950Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_CANCELLATION_PAGE = 'Cancellation Confirmation'\n",
    "REGISTRATION_PAGE = 'Submit Registration'\n",
    "milliseconds_to_hours = 3600 * 1000\n",
    "minutes_to_hours = 60 * 60\n",
    "TRUE = 1\n",
    "FALSE = 0\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \n",
    "    info('Starting data cleaning...')\n",
    "    \n",
    "    total_before = df.count()\n",
    "    \n",
    "    # Keep only logged records\n",
    "    df = df.where(df.auth.isin(['Logged In', 'Cancelled']))\n",
    "    \n",
    "    # Records without userId\n",
    "    df = df.where(col('userId').isNotNull())\n",
    "    \n",
    "    # Create a date column for the event\n",
    "    df = df.withColumn('date', from_unixtime(col('ts') / 1000).cast(DateType()))\n",
    "    \n",
    "    # Location\n",
    "    # df = df.withColumn('state', trim(split((split('location', ',').getItem(1)), '-').getItem(0)))\n",
    "    \n",
    "    # Relevant windows\n",
    "    w_session = Window.partitionBy('sessionId').orderBy('ts')\n",
    "    w_user_session = Window.partitionBy('sessionId', 'userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    w_user = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # Create features\n",
    "    df = df.withColumn('previous_page', lag(df.page).over(w_session))\n",
    "    df = df.withColumn('last_event_ts', last(col('ts')).over(w_user))\n",
    "    df = df.withColumn('last_page', last(col('page')).over(w_user))\n",
    "    df = df.withColumn('register_page', first(col('previous_page')).over(w_user))\n",
    "    df = df.withColumn('first_ts', first(col('ts')).over(w_user))\n",
    "    df = df.withColumn('ts_elapsed', last(df.ts).over(w_session) - first(df.ts).over(w_user_session))\n",
    "    df = df.withColumn('session_duration', smax(df.ts_elapsed).over(w_user_session))\n",
    "     \n",
    "    info('Finished data cleaning...')\n",
    "    info(f'Number of removed rows: {total_before - df.count()}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:11.252807Z",
     "start_time": "2020-03-21T20:48:08.038640Z"
    }
   },
   "outputs": [],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:01.910759Z",
     "start_time": "2020-03-08T18:30:01.480161Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions about the data:\n",
    "\n",
    "- Are errors related to downgrading canceling the service?\n",
    "- Having a certain number of friends or a sense of community can decrease the churn?\n",
    "- Thumbs down are related to churn? (could the quality of the songs catalog affect the churn)\n",
    "- The advertising is not annoying the users?\n",
    "- Users with stay connected for more time have less change to churn?\n",
    "- Is the home page relevant?\n",
    "- Users, who access the downgrade page are how much more willing to churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:06.259628Z",
     "start_time": "2020-03-08T18:30:05.841058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('status').count().orderBy('count', ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T22:44:01.657485Z",
     "start_time": "2020-03-01T22:44:01.094695Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:16.616860Z",
     "start_time": "2020-03-08T18:30:16.073824Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:29.650593Z",
     "start_time": "2020-03-08T18:30:29.435808Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('userAgent').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:37.323100Z",
     "start_time": "2020-03-08T18:30:36.828209Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92 and song != \\'null\\' ').groupBy('song').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:34.695716Z",
     "start_time": "2020-03-21T20:48:34.664329Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_session_dimension(df):\n",
    "    \n",
    "    # sessions from the user\n",
    "    df_sessions = df.orderBy(df.sessionId).groupBy('sessionId', 'userId').agg(\n",
    "        smax(df.ts).alias('max_event_ts'),\n",
    "        smin(df.ts).alias('min_event_ts'),\n",
    "        ssum(df.length).alias('session_n_total_playback'), # Based on songs length\n",
    "        count(when(df.page == 'Thumbs Up', True)).alias(\"session_n_likes\"),\n",
    "        count(when(df.page == 'Thumbs Down', True)).alias(\"session_n_dislikes\"),\n",
    "        count(when(df.page == 'NextSong', True)).alias(\"session_n_songs\"),\n",
    "        count(when(df.page == 'Add Friend', True)).alias(\"session_n_friends\"),\n",
    "        count(when(df.page == 'Add to Playlist', True)).alias(\"session_n_add_playlist\"),\n",
    "        count(when(df.page == 'Home', True)).alias(\"session_n_home\"),\n",
    "        count(when(df.page == 'Roll Advert', True)).alias(\"session_n_ads\"),\n",
    "        count(when(df.page == 'Help', True)).alias(\"session_n_help\"),\n",
    "        count(when(df.page == 'Error', True)).alias(\"session_n_error\"),\n",
    "        count(when(df.page == 'Settings', True)).alias(\"session_n_sets\"),\n",
    "        count(col('page')).alias('session_n_actions'),\n",
    "        first(col('session_duration')).alias('session_duration')\n",
    "    ) \n",
    "    \n",
    "    # Calculate the interval until the next session\n",
    "    w_user_sessions_interval = Window.partitionBy('userId').orderBy('min_event_ts')\n",
    "    df_sessions = df_sessions.withColumn('interval_to_session', col('min_event_ts') - lag(col('max_event_ts')).over(w_user_sessions_interval))\n",
    "    \n",
    "    # Calculate average time in hours for each session\n",
    "    df_session_time = df_sessions.groupBy('userId').agg(\n",
    "       (avg(df_sessions.session_duration) / milliseconds_to_hours).alias('session_hours')\n",
    "    )\n",
    "    df_sessions = df_sessions.join(df_session_time, on = 'userId')\n",
    "    \n",
    "    # We should remove the null lines before count/group to not account 2 times the mean interval\n",
    "    df_sessions = df_sessions.groupBy('userId').agg(  \n",
    "        (avg(df_sessions.interval_to_session) / milliseconds_to_hours).alias('session_avg_time_away'),\n",
    "        ((avg(df_sessions.session_n_total_playback) / minutes_to_hours) / first(col('session_hours'))).alias('session_avg_playback'), \n",
    "        (avg(df_sessions.session_n_likes) / first(col('session_hours'))).alias('session_avg_likes'),\n",
    "        (avg(df_sessions.session_n_dislikes) / first(col('session_hours'))).alias('session_avg_dislikes'),\n",
    "        (avg(df_sessions.session_n_songs) / first(col('session_hours'))).alias('session_avg_songs'),\n",
    "        (avg(df_sessions.session_n_friends) / first(col('session_hours'))).alias('session_avg_friends'),\n",
    "        (avg(df_sessions.session_n_add_playlist) / first(col('session_hours'))).alias('session_avg_added_playlist'),\n",
    "        (avg(df_sessions.session_n_home) / first(col('session_hours'))).alias('session_avg_home'),\n",
    "        (avg(df_sessions.session_n_ads) / first(col('session_hours'))).alias('session_avg_ads'),\n",
    "        (avg(df_sessions.session_n_help) / first(col('session_hours'))).alias('session_avg_help'),\n",
    "        (avg(df_sessions.session_n_error) / first(col('session_hours'))).alias('session_avg_errors'),\n",
    "        (avg(df_sessions.session_n_sets) / first(col('session_hours'))).alias('session_avg_settings'),\n",
    "        (avg(df_sessions.session_n_actions) / first(col('session_hours'))).alias('session_avg_actions')\n",
    "    )\n",
    "    \n",
    "    return df_sessions\n",
    "\n",
    "def create_user_dimension(df):\n",
    "    \n",
    "    df_user_profile = df.groupby('userId')\\\n",
    "        .agg( \n",
    "\n",
    "            # first(col('state')).alias('state'),\n",
    "            first(when(col('gender') == 'M', TRUE).otherwise(FALSE)).alias('male'),\n",
    "\n",
    "            smin(col('first_ts')).alias('ts_start'),\n",
    "            smax(col('last_event_ts')).alias('ts_end'),        \n",
    "        \n",
    "            ((smax(col('last_event_ts')) - smin(col('first_ts'))) / milliseconds_to_hours).alias('time_window'),\n",
    "        \n",
    "            # Subscription\n",
    "            count(when(col('page') == 'Submit Downgrade', True)).alias('n_downgrades'),\n",
    "            count(when(col('page') == 'Submit Upgrade', True)).alias('n_upgrades'),\n",
    "            last(when(col('level') == 'paid', TRUE).otherwise(FALSE)).alias('paid'),\n",
    "            first(when(col('last_page') == CHURN_CANCELLATION_PAGE, TRUE).otherwise(FALSE)).alias('canceled'),\n",
    "\n",
    "            # Streaming\n",
    "            count(when(col('page') == 'NextSong', True)).alias('n_songs'),\n",
    "            count(when(col('page') == 'Thumbs Up', True)).alias('n_likes'),\n",
    "            count(when(col('page') == 'Thumbs Down', True)).alias('n_dislikes'),\n",
    "            countDistinct(col('sessionId')).alias('n_sess'),\n",
    "            (avg(col('session_duration')) / milliseconds_to_hours).alias('avg_session_duration'),\n",
    "\n",
    "            # Community\n",
    "            count(when(col('page') == 'Add Friend', True)).alias('n_friends'),\n",
    "            count(when(col('page') == 'Add to Playlist', True)).alias('n_added_to_playlist'),\n",
    "\n",
    "            # Other\n",
    "            count(when(col('page') == 'Home', True)).alias('n_home'),\n",
    "            count(when(col('page') == 'Roll Advert', True)).alias('n_ads'),\n",
    "            count(when(col('page') == 'Help', True)).alias('n_help'),\n",
    "            count(when(col('page') == 'Error', True)).alias('n_errors'),\n",
    "            count(when(col('page') == 'Settings', True)).alias('n_settings'),\n",
    "            count(col('page')).alias('n_actions')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Location\n",
    "    # states = list(map(lambda c: c[0].strip(), df.select(['state']).distinct().rdd.collect()))\n",
    "    # for state in states:\n",
    "    #    df_user_profile = df_user_profile.withColumn(state.lower(), when(df_user_profile.state == state, 1).otherwise(0))\n",
    "    \n",
    "    return df_user_profile\n",
    "\n",
    "def create_days_dimension(df):\n",
    "    \n",
    "    df_unique_days = df.groupby('userId').agg(countDistinct('date').alias('n_days'))\n",
    "    \n",
    "    df_daily_actions = df.groupby('userId', 'date').agg(count('page').alias('total'))\n",
    "    df_daily_actions = df_daily_actions.groupby('userId').agg(avg('total').alias('avg_daily_actions')) \n",
    "\n",
    "    df_days = df_unique_days.join(df_daily_actions, df_unique_days.userId == df_daily_actions.userId)\n",
    "    \n",
    "    # Remove duplicated column after join\n",
    "    df_days = df_days.drop(df_daily_actions.userId)\n",
    "    \n",
    "    return df_days\n",
    "\n",
    "def sort_features(df, columns_order):\n",
    "    _columns = df.columns\n",
    "    _columns.sort()\n",
    "    \n",
    "    for _idx, _val in list(enumerate(columns_order)):\n",
    "        _columns.pop(_columns.index(_val))\n",
    "        _columns.insert(_idx, _val)\n",
    "        \n",
    "    assert len(_columns) == len(df.columns)\n",
    "\n",
    "    return _columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:38.808116Z",
     "start_time": "2020-03-21T20:48:38.055995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sessions = create_session_dimension(df)\n",
    "df_days = create_days_dimension(df)\n",
    "\n",
    "df_users = create_user_dimension(df)\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_days, on = 'userId')\n",
    "\n",
    "_columns = sort_features(df_users, [ 'userId', 'male', 'paid', 'canceled'])\n",
    "_columns = list(set(df_users.schema.names + df_sessions.schema.names) - set(['ts_start', 'ts_end', 'state']))\n",
    "\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:48:42.386121Z",
     "start_time": "2020-03-21T20:48:42.376902Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:03.344898Z",
     "start_time": "2020-03-21T20:48:49.766564Z"
    }
   },
   "outputs": [],
   "source": [
    "### WARN: Only round to display\n",
    "# Enforces the order for some columns\n",
    "df_users.select([sround(c, 0).cast(dataType = IntegerType()).alias(c) for c in _columns]).fillna(0).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:25.483187Z",
     "start_time": "2020-03-21T20:49:15.575158Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.select(_columns).fillna(0).toPandas().to_csv('sparkify_data_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:53.790091Z",
     "start_time": "2020-03-21T20:49:53.293466Z"
    }
   },
   "outputs": [],
   "source": [
    "df.agg(countDistinct(df.userId).alias('unique_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).groupBy('canceled').agg(count(df_users.canceled).alias('total')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advertises number (per session and all)\n",
    "    - The user **100010** returned after some idle time and received a considerable amount of advertises;\n",
    "    - Also, after thumbs down, I received two advertisements on four sounds. Then canceled the service.\n",
    "- Number of sessions\n",
    "- Paid subscription time\n",
    "- Avg songs before an ad\n",
    "- Number of skipped songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_date(df.ts.cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.userId == user_id).select(['artist',\n",
    " 'auth',\n",
    " 'firstName',\n",
    " 'gender',\n",
    " 'itemInSession',\n",
    " 'lastName',\n",
    " 'length',\n",
    " 'level', \n",
    " 'page',\n",
    " 'sessionId',\n",
    " 'song', \n",
    " 'ts', \n",
    " 'userId']).orderBy('sessionId', 'itemInSession').withColumn('datetime', date_format((df.ts/1000).cast(dataType=TimestampType()), 'HH:mm:ss dd-MM-YYYY')).show(350, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:01:33.361968Z",
     "start_time": "2020-03-01T15:01:33.311321Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:01:34.745712Z",
     "start_time": "2020-03-01T15:01:34.737694Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_exclude = set(['userId'])\n",
    "\n",
    "columns_to_use = list(set(df_users.columns) - columns_to_exclude)\n",
    "\n",
    "columns_to_train = list(set(columns_to_use) - set(['canc']))\n",
    "\n",
    "columns_to_use.sort()\n",
    "columns_to_train.sort()\n",
    "\n",
    "print(f'Columns: {columns_to_use}\\n')\n",
    "print(f'Columns to train: {columns_to_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:35:56.285672Z",
     "start_time": "2020-03-21T21:35:56.281724Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_LABEL = 'canceled'\n",
    "TRAIN_SPLIT_RATIO = .7\n",
    "TEST_SPLIT_RATIO = .3\n",
    "\n",
    "SPLIT_RATIO = [TRAIN_SPLIT_RATIO, TEST_SPLIT_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:00:27.552585Z",
     "start_time": "2020-03-21T23:00:27.540050Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_features = [ 'paid', 'male' ]\n",
    "\n",
    "numeric_features = [\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_all = [\n",
    "    'canceled',\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_to_train = [\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:13:22.270389Z",
     "start_time": "2020-03-21T23:13:22.247433Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def evaluate_multiclass_classifier(predictions, columns):\n",
    "    metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall' ]\n",
    "    \n",
    "    result = {}\n",
    "    for metric in metrics_to_evaluate:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol = columns[0], predictionCol = columns[1], metricName = metric)\n",
    "        value = evaluator.evaluate(predictions)\n",
    "        result[metric] = value\n",
    "        print(f'{metric}: {value}') \n",
    "    \n",
    "    return result\n",
    "\n",
    "# f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "#    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "\n",
    "def train_random_forest_classifier(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10, maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42)\n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, rf_classifier, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "    \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, model ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def create_random_forest_pipeline():\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'canceled', featuresCol = 'features', seed = 42)\n",
    "    return create_pipeline(rf_classifier)\n",
    "\n",
    "def create_gradient_boost_pipeline():\n",
    "    gbt_classifier = GBTClassifier(labelCol = 'canceled', maxDepth = 5, maxIter = 100)\n",
    "    return create_pipeline(gbt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:06:32.714838Z",
     "start_time": "2020-03-21T21:06:32.602035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "df_users = df_users.select(columns_all).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:07:01.999691Z",
     "start_time": "2020-03-21T21:06:53.158970Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:14:23.154867Z",
     "start_time": "2020-03-21T23:13:27.220280Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:15:33.676310Z",
     "start_time": "2020-03-21T23:14:48.782886Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(predictions, ('canceled', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:15:59.264578Z",
     "start_time": "2020-03-21T23:15:49.342154Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction', 'predictedLabel']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:54:56.121279Z",
     "start_time": "2020-03-21T21:54:56.113194Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:38:50.323684Z",
     "start_time": "2020-03-21T21:38:50.316950Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:33:56.261503Z",
     "start_time": "2020-03-21T21:33:55.472160Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:38:39.228859Z",
     "start_time": "2020-03-21T21:38:39.213960Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:34:57.823584Z",
     "start_time": "2020-03-21T21:34:57.817689Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T21:34:10.817762Z",
     "start_time": "2020-03-21T21:34:10.808986Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_predictions):\n",
    "    \n",
    "    # auc = roc_auc_score(y_test, y_predictions)\n",
    "    cm = confusion_matrix(y_test, y_predictions, labels = [1, 0])\n",
    "    \n",
    "    tn = cm[0, 0]\n",
    "    tp = cm[1, 1]\n",
    "    fn = cm[1, 0]\n",
    "    fp = cm[0, 1]\n",
    "    \n",
    "    total = np.sum(cm) # tn + tp + fn + fp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn) \n",
    "    \n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:27:07.966315Z",
     "start_time": "2020-02-29T21:26:10.470241Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_to_use, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:49:02.428480Z",
     "start_time": "2020-02-29T21:47:25.148792Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(predictions, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:49:40.743405Z",
     "start_time": "2020-02-29T21:49:22.720697Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:46:24.619400Z",
     "start_time": "2020-03-21T23:46:24.598366Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def create_grid_search(pipeline, param_grid, evaluator = MulticlassClassificationEvaluator(labelCol = 'canceled', metricName = 'f1')):\n",
    "    \n",
    "    return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = evaluator, numFolds = 2, parallelism = 10)\n",
    "\n",
    "def random_forest_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_rf = ParamGridBuilder().addGrid(model.maxDepth, [5, 10, 15, 20, 25]) \n",
    "    grid_rf = grid_rf.addGrid(model.impurity, ['gini']) \n",
    "    grid_rf = grid_rf.addGrid(model.maxBins, list(range(5, 45, )))\n",
    "    grid_rf = grid_rf.addGrid(model.numTrees, [10, 20, 40, 60, 70])\n",
    "    grid_rf = grid_rf.build()\n",
    "        \n",
    "    return create_grid_search(pipeline, grid_rf)\n",
    "\n",
    "def gradient_boost_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_gbt = ParamGridBuilder().addGrid(model.maxDepth, [5]) #, 10, 15, 20, 25])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxIter, [20])#, 25, 40, 50, 100])\n",
    "    grid_gbt = grid_gbt.build()\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:54:38.113717Z",
     "start_time": "2020-03-21T22:54:38.106941Z"
    }
   },
   "outputs": [],
   "source": [
    "[int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:02:25.586545Z",
     "start_time": "2020-03-21T22:02:25.570112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "# data = df_users.select(columns_to_use).fillna(0)\n",
    "\n",
    "# Split train/test\n",
    "(train_df, test_df) = df_users.randomSplit(SPLIT_RATIO, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:01:50.804573Z",
     "start_time": "2020-03-01T15:01:50.217634Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:00:32.430502Z",
     "start_time": "2020-03-21T23:47:01.083869Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)\n",
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:16:33.018711Z",
     "start_time": "2020-03-22T02:16:33.006359Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def get_grid_iterable():\n",
    "    param_grid = {'learning_rate': [float(v) for v in np.arange(0.01, 0.25, 0.01)],\n",
    "                  'colsample_bytree': [float(v) for v in np.arange(0.8, 1.01, 0.1)],\n",
    "                  'subsample': [float(v) for v in np.arange(0.5, 1.01, 0.1)],\n",
    "                  'n_estimators': [int(v) for v in np.arange(100, 3000, 100)],\n",
    "                  'reg_alpha': [float(v) for v in np.arange(0.01, 0.5, 0.05)],\n",
    "                  'max_depth': [int(v) for v in np.arange(3, 14, 1)],\n",
    "                  'gamma': [int(v) for v in np.arange(0, 10, 2)]\n",
    "                  }\n",
    "    \n",
    "    grid_iter = []\n",
    "    length = 1\n",
    "    for k in param_grid:\n",
    "        grid_iter.append(param_grid[k])\n",
    "        length *= len(param_grid[k])\n",
    "\n",
    "    return itertools.product(*grid_iter), list(param_grid.keys()), length-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:19:32.439535Z",
     "start_time": "2020-03-22T02:19:32.397409Z"
    }
   },
   "outputs": [],
   "source": [
    "rfmodel = RandomForestClassifier()\n",
    "\n",
    "grid_rf = ParamGridBuilder().addGrid(rfmodel.maxDepth, [5, 10, 15, 20, 25]) \n",
    "grid_rf = grid_rf.addGrid(rfmodel.impurity, ['gini']) \n",
    "grid_rf = grid_rf.addGrid(rfmodel.maxBins, list(range(5, 45, )))\n",
    "grid_rf = grid_rf.addGrid(rfmodel.numTrees, [10, 20, 40, 60, 70])\n",
    "grid_rf = grid_rf.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:16:52.615098Z",
     "start_time": "2020-03-22T02:16:52.609616Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "get_grid_iterable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:15:49.902263Z",
     "start_time": "2020-03-22T02:15:49.886031Z"
    }
   },
   "outputs": [],
   "source": [
    "from generate_grid import get_grid_iterable # The garid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from interruptingcow import timeout\n",
    "from sklearn.model_selection import KFold  # import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import json\n",
    "from preprocessing import preprocessing, xgb_r2_score # The preprocessing and the r2 evaluation\n",
    "from generate_grid import get_grid_iterable # The grid\n",
    "\n",
    "def grid_search(timeout_seconds, cv_splits, boosting_rounds):\n",
    "    #   Read input data\n",
    "    X, y = preprocessing()\n",
    "\n",
    "    #   Create dataframe to collect the results\n",
    "    tests_columns = [\"test_nr\", \"cv_mean\", \"cv_min\", \"cv_max\", \"cv_median\", \"params\"]\n",
    "    test_id = 0\n",
    "    tests = pd.DataFrame(columns=tests_columns)\n",
    "\n",
    "    #   Cross validation number of splits\n",
    "    kf = KFold(n_splits=cv_splits)\n",
    "\n",
    "    #   Execute until timeout occurs\n",
    "    with timeout(timeout_seconds, exception=RuntimeError):\n",
    "\n",
    "        #   Get the grid\n",
    "        grid_iter, keys, length = get_grid_iterable()\n",
    "        try:\n",
    "\n",
    "            #   For every element of the grid\n",
    "            for df_grid in grid_iter:\n",
    "                #   Prepare a list to collect the scores\n",
    "                score = []\n",
    "                params = dict(zip(keys, df_grid))\n",
    "\n",
    "                #   The objective function\n",
    "                params[\"objective\"] = \"reg:squarederror\"\n",
    "\n",
    "                #   For each fold, train XGBoost and spit out the results\n",
    "                for train_index, test_index in kf.split(X.values):\n",
    "\n",
    "                    #   Get X train and X test\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "                    #   Get y train and y test\n",
    "                    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    #   Convert into DMatrix\n",
    "                    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "                    d_valid = xgb.DMatrix(X_test, label=y_test)\n",
    "                    d_test = xgb.DMatrix(X_test)\n",
    "                    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "                    #   Create the classifier using the current grid params. Apply early stopping of 50 rounds\n",
    "                    clf = xgb.train(params, d_train, boosting_rounds, watchlist, early_stopping_rounds=50,\n",
    "                                    feval=xgb_r2_score, maximize=True, verbose_eval=10)\n",
    "                    y_hat = clf.predict(d_test)\n",
    "\n",
    "                    #   Append Scores on the fold kept out\n",
    "                    score.append(r2_score(y_test, y_hat))\n",
    "\n",
    "                #   Store the result into a dataframe\n",
    "                score_df = pd.DataFrame(columns=tests_columns, data=[\n",
    "                    [test_id, np.mean(score), np.min(score), np.max(score), np.median(score),\n",
    "                     json.dumps(dict(zip(keys, [str(g) for g in df_grid])))]])\n",
    "                test_id += 1\n",
    "                tests = pd.concat([tests, score_df])\n",
    "                \n",
    "        except RuntimeError:\n",
    "            #   When timeout occurs an exception is raised and the main cycle is broken\n",
    "            pass\n",
    "\n",
    "    #   Spit out the results\n",
    "    tests.to_csv(\"grid-search.csv\", index=False)\n",
    "    print(tests)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(timeout_seconds=3600, cv_splits=4, boosting_rounds=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T02:01:52.049270Z",
     "start_time": "2020-03-22T02:01:51.673624Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:48:51.783153Z",
     "start_time": "2020-03-21T22:48:51.778273Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10)\n",
    "dir(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:04:06.345577Z",
     "start_time": "2020-03-21T23:04:06.331164Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:43:55.011960Z",
     "start_time": "2020-03-21T22:43:45.931376Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:44:06.060150Z",
     "start_time": "2020-03-21T22:43:56.785468Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:11:05.946121Z",
     "start_time": "2020-03-21T22:10:56.684420Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T22:07:09.309168Z",
     "start_time": "2020-03-21T22:07:09.304265Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    " \n",
    "best_model_results = cv_gbt_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T15:02:01.341210Z",
     "start_time": "2020-03-01T15:02:01.298441Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T18:08:34.371481Z",
     "start_time": "2020-03-01T15:02:03.210992Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_gbt_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:39.482836Z",
     "start_time": "2020-03-01T14:06:39.477907Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_gbt_results.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:56.517560Z",
     "start_time": "2020-03-01T14:06:56.488834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = cv_gbt_results.avgMetrics\n",
    "params = [{p.name: v for p, v in m.items()} for m in cv_gbt.getEstimatorParamMaps()]\n",
    "params_pd = pd.DataFrame(params)\n",
    "params_pd['score'] = scores\n",
    "params_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:00:50.268594Z",
     "start_time": "2020-03-01T03:00:44.103376Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'canc', metricName = 'areaUnderROC')\n",
    " \n",
    "best_model_results = cv_gbt_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T23:35:59.822421Z",
     "start_time": "2020-03-21T23:35:39.991922Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluator = BinaryClassificationEvaluator(labelCol = 'canceled', metricName = 'f1Measure')\n",
    "\n",
    "metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall' ]\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'canceled', metricName = 'f1')\n",
    " \n",
    "best_model_results = cv_rf_results.bestModel.transform(test_df)\n",
    "    \n",
    "evaluator.evaluate(best_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:01:23.359069Z",
     "start_time": "2020-03-01T03:01:19.905831Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(best_model_results, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:39:49.645897Z",
     "start_time": "2020-03-01T00:38:10.913242Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_multiclass_classifier(best_model_results, ('canc', 'prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:13:53.940084Z",
     "start_time": "2020-03-01T00:13:40.288881Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(['features', 'prediction', 'canc']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T22:56:07.000157Z",
     "start_time": "2020-02-29T22:55:53.705530Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(['rawPrediction', 'prediction', 'canc']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:06:14.995253Z",
     "start_time": "2020-02-29T21:05:57.745177Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.filter('canc = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T21:06:44.651611Z",
     "start_time": "2020-02-29T21:06:27.509694Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.filter('canc = 1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T03:01:53.168730Z",
     "start_time": "2020-03-01T03:01:52.310500Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model_results.select(\"prediction\", \"canc\", \"features\").filter('canc = 1').groupby(['canc', 'prediction']).agg({'canc':'count'}).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:47:05.176261Z",
     "start_time": "2020-03-01T00:47:05.169056Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
