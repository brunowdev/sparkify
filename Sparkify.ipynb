{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:50:54.906756Z",
     "start_time": "2020-05-03T18:50:54.817021Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "# findspark.init('/home/brunowdev/spark-2.4.5-bin-hadoop2.6/')\n",
    "\n",
    "findspark.init('/home/bruno/LIBS/spark')\n",
    "\n",
    "import evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:14:15.092138Z",
     "start_time": "2020-05-03T19:14:15.079038Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import min as smin, max as smax, sum as ssum, round as sround, abs as sabs, pow as spow\n",
    "from pyspark.sql.functions import isnan, isnull, when, first, avg, sqrt, last, count, countDistinct, col, lag, lead, coalesce, lit, split, trim\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_date, date_format, from_unixtime, to_timestamp\n",
    "\n",
    "from pyspark.sql.types import DateType, TimestampType, IntegerType\n",
    " \n",
    "import jupyter_utils as j\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, MaxAbsScaler, Normalizer, MinMaxScaler, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, Evaluator\n",
    "from pyspark import since, keyword_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark():\n",
    "     \n",
    "    SparkContext.setSystemProperty('spark.logConf', 'True')\n",
    "    SparkContext.setSystemProperty('spark.default.parallelism', '16')\n",
    "    SparkContext.setSystemProperty('spark.executor.memory', '4g')\n",
    "    SparkContext.setSystemProperty('spark.driver.memory', '8g')\n",
    "    SparkContext.setSystemProperty('spark.reducer.maxSizeInFlight', '96m')\n",
    "    SparkContext.setSystemProperty('spark.shuffle.consolidateFiles', 'True') \n",
    "    SparkContext.setSystemProperty('spark.shuffle.service.index.cache.size', '500m')\n",
    "\n",
    "    SparkContext.setSystemProperty('spark.driver.extraJavaOptions', '-server -Xmx8G')\n",
    "    # SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-server -Xmx8G -XX:+UseG1GC')\n",
    "\n",
    "    SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-server -XX:+UseG1GC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:00:24.505080Z",
     "start_time": "2020-05-03T19:00:24.499240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "j.reload(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:51:04.342727Z",
     "start_time": "2020-05-03T18:51:04.334447Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = 'sparkify_full_csv_data.csv'\n",
    "# filepath = 'medium_sparkify_event_data.json'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:51:06.038370Z",
     "start_time": "2020-05-03T18:51:06.013026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '39281'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.app.id', 'local-1588530423385'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'Sparkify'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.0.102'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:51:22.284813Z",
     "start_time": "2020-05-03T18:51:11.468483Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").csv(filepath)\n",
    "# df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"encoding\", \"utf-8\").json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T22:32:41.168357Z",
     "start_time": "2020-05-02T22:32:41.111933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gender: string, length: double, level: string, registration: double, userId: int, ts: bigint, page: string, sessionId: int, itemInSession: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T18:08:51.472230Z",
     "start_time": "2020-05-02T18:08:19.478102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "|gender|length|level|registration|userId| ts|page|sessionId|itemInSession|\n",
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "+------+------+-----+------------+------+---+----+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.userId == 100010).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:51:26.145880Z",
     "start_time": "2020-05-03T18:51:26.135663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger instance created\n"
     ]
    }
   ],
   "source": [
    "log4jLogger = spark.sparkContext._jvm.org.apache.log4j\n",
    "\n",
    "LOGGER = log4jLogger.LogManager.getLogger('driver_logger')\n",
    "\n",
    "def info(message, print_on_notebook = True):\n",
    "    LOGGER.info(message)\n",
    "    \n",
    "    if print_on_notebook:\n",
    "        print(message)\n",
    "    \n",
    "info('Logger instance created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:51:58.941213Z",
     "start_time": "2020-05-03T18:51:58.911502Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "def set_storage_on_memory():\n",
    "    info(df.storageLevel)\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)\n",
    "    info(df.storageLevel)\n",
    "    \n",
    "set_storage_on_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:53:34.680286Z",
     "start_time": "2020-05-03T18:53:34.674338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:53:42.893648Z",
     "start_time": "2020-05-03T18:53:42.877944Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_CANCELLATION_PAGE = 'Cancellation Confirmation'\n",
    "REGISTRATION_PAGE = 'Submit Registration'\n",
    "milliseconds_to_hours = 3600 * 1000\n",
    "minutes_to_hours = 60 * 60\n",
    "TRUE = 1\n",
    "FALSE = 0\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \n",
    "    info('Starting data cleaning...')\n",
    "    \n",
    "    total_before = df.count()\n",
    "    \n",
    "    # Keep only logged records\n",
    "    # df = df.where(df.auth.isin(['Logged In', 'Cancelled']))\n",
    "    \n",
    "    # Records without userId\n",
    "    df = df.where(col('userId').isNotNull())\n",
    "    \n",
    "    # Create a date column for the event\n",
    "    df = df.withColumn('date', from_unixtime(col('ts') / 1000).cast(DateType()))\n",
    "    \n",
    "    # Location\n",
    "    # df = df.withColumn('state', trim(split((split('location', ',').getItem(1)), '-').getItem(0)))\n",
    "    \n",
    "    # Relevant windows\n",
    "    w_session = Window.partitionBy('sessionId').orderBy('ts')\n",
    "    w_user_session = Window.partitionBy('sessionId', 'userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    w_user = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # Create features\n",
    "    df = df.withColumn('previous_page', lag(df.page).over(w_session))\n",
    "    df = df.withColumn('last_event_ts', last(col('ts')).over(w_user))\n",
    "    df = df.withColumn('last_page', last(col('page')).over(w_user))\n",
    "    df = df.withColumn('register_page', first(col('previous_page')).over(w_user))\n",
    "    df = df.withColumn('first_ts', first(col('ts')).over(w_user))\n",
    "    df = df.withColumn('ts_elapsed', last(df.ts).over(w_session) - first(df.ts).over(w_user_session))\n",
    "    df = df.withColumn('session_duration', smax(df.ts_elapsed).over(w_user_session))\n",
    "     \n",
    "    info('Finished data cleaning...')\n",
    "    info(f'Number of removed rows: {total_before - df.count()}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:01.655716Z",
     "start_time": "2020-05-03T18:53:45.106348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Finished data cleaning...\n",
      "Number of removed rows: 0\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T17:45:03.557357Z",
     "start_time": "2020-05-02T17:44:58.623030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                page|   count|\n",
      "+--------------------+--------+\n",
      "|            NextSong|20850272|\n",
      "|                Home| 1343102|\n",
      "|           Thumbs Up| 1151465|\n",
      "|     Add to Playlist|  597921|\n",
      "|         Roll Advert|  385212|\n",
      "|          Add Friend|  381664|\n",
      "|               Login|  296350|\n",
      "|              Logout|  296005|\n",
      "|         Thumbs Down|  239212|\n",
      "|           Downgrade|  184240|\n",
      "|                Help|  155100|\n",
      "|            Settings|  147074|\n",
      "|               About|   92759|\n",
      "|             Upgrade|   50507|\n",
      "|       Save Settings|   29516|\n",
      "|               Error|   25962|\n",
      "|      Submit Upgrade|   15135|\n",
      "|    Submit Downgrade|    6494|\n",
      "|Cancellation Conf...|    5003|\n",
      "|              Cancel|    5003|\n",
      "|            Register|     802|\n",
      "| Submit Registration|     401|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions about the data:\n",
    "\n",
    "- Are errors related to downgrading canceling the service?\n",
    "- Having a certain number of friends or a sense of community can decrease the churn?\n",
    "- Thumbs down are related to churn? (could the quality of the songs catalog affect the churn)\n",
    "- The advertising is not annoying the users?\n",
    "- Users with stay connected for more time have less change to churn?\n",
    "- Is the home page relevant?\n",
    "- Users, who access the downgrade page are how much more willing to churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:06.259628Z",
     "start_time": "2020-03-08T18:30:05.841058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('status').count().orderBy('count', ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T22:44:01.657485Z",
     "start_time": "2020-03-01T22:44:01.094695Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:16.616860Z",
     "start_time": "2020-03-08T18:30:16.073824Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('page').count().orderBy('count', ascending = False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:29.650593Z",
     "start_time": "2020-03-08T18:30:29.435808Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92').groupBy('userAgent').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:30:37.323100Z",
     "start_time": "2020-03-08T18:30:36.828209Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter('userId = 92 and song != \\'null\\' ').groupBy('song').count().orderBy('count', ascending = False).show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:13.971859Z",
     "start_time": "2020-05-03T18:54:13.939951Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_session_dimension(df):\n",
    "    \n",
    "    # sessions from the user\n",
    "    df_sessions = df.orderBy(df.sessionId).groupBy('sessionId', 'userId').agg(\n",
    "        smax(df.ts).alias('max_event_ts'),\n",
    "        smin(df.ts).alias('min_event_ts'),\n",
    "        ssum(df.length).alias('session_n_total_playback'), # Based on songs length\n",
    "        count(when(df.page == 'Thumbs Up', True)).alias(\"session_n_likes\"),\n",
    "        count(when(df.page == 'Thumbs Down', True)).alias(\"session_n_dislikes\"),\n",
    "        count(when(df.page == 'NextSong', True)).alias(\"session_n_songs\"),\n",
    "        count(when(df.page == 'Add Friend', True)).alias(\"session_n_friends\"),\n",
    "        count(when(df.page == 'Add to Playlist', True)).alias(\"session_n_add_playlist\"),\n",
    "        count(when(df.page == 'Home', True)).alias(\"session_n_home\"),\n",
    "        count(when(df.page == 'Roll Advert', True)).alias(\"session_n_ads\"),\n",
    "        count(when(df.page == 'Help', True)).alias(\"session_n_help\"),\n",
    "        count(when(df.page == 'Error', True)).alias(\"session_n_error\"),\n",
    "        count(when(df.page == 'Settings', True)).alias(\"session_n_sets\"),\n",
    "        count(col('page')).alias('session_n_actions'),\n",
    "        first(col('session_duration')).alias('session_duration')\n",
    "    ) \n",
    "    \n",
    "    # Calculate the interval until the next session\n",
    "    w_user_sessions_interval = Window.partitionBy('userId').orderBy('min_event_ts')\n",
    "    df_sessions = df_sessions.withColumn('interval_to_session', col('min_event_ts') - lag(col('max_event_ts')).over(w_user_sessions_interval))\n",
    "    \n",
    "    # Calculate average time in hours for each session\n",
    "    df_session_time = df_sessions.groupBy('userId').agg(\n",
    "       (avg(df_sessions.session_duration) / milliseconds_to_hours).alias('session_hours')\n",
    "    )\n",
    "    df_sessions = df_sessions.join(df_session_time, on = 'userId')\n",
    "    \n",
    "    # We should remove the null lines before count/group to not account 2 times the mean interval\n",
    "    df_sessions = df_sessions.groupBy('userId').agg(  \n",
    "        (avg(df_sessions.interval_to_session) / milliseconds_to_hours).alias('session_avg_time_away'),\n",
    "        ((avg(df_sessions.session_n_total_playback) / minutes_to_hours) / first(col('session_hours'))).alias('session_avg_playback'), \n",
    "        (avg(df_sessions.session_n_likes) / first(col('session_hours'))).alias('session_avg_likes'),\n",
    "        (avg(df_sessions.session_n_dislikes) / first(col('session_hours'))).alias('session_avg_dislikes'),\n",
    "        (avg(df_sessions.session_n_songs) / first(col('session_hours'))).alias('session_avg_songs'),\n",
    "        (avg(df_sessions.session_n_friends) / first(col('session_hours'))).alias('session_avg_friends'),\n",
    "        (avg(df_sessions.session_n_add_playlist) / first(col('session_hours'))).alias('session_avg_added_playlist'),\n",
    "        (avg(df_sessions.session_n_home) / first(col('session_hours'))).alias('session_avg_home'),\n",
    "        (avg(df_sessions.session_n_ads) / first(col('session_hours'))).alias('session_avg_ads'),\n",
    "        (avg(df_sessions.session_n_help) / first(col('session_hours'))).alias('session_avg_help'),\n",
    "        (avg(df_sessions.session_n_error) / first(col('session_hours'))).alias('session_avg_errors'),\n",
    "        (avg(df_sessions.session_n_sets) / first(col('session_hours'))).alias('session_avg_settings'),\n",
    "        (avg(df_sessions.session_n_actions) / first(col('session_hours'))).alias('session_avg_actions')\n",
    "    )\n",
    "    \n",
    "    return df_sessions\n",
    "\n",
    "def create_user_dimension(df):\n",
    "    \n",
    "    df_user_profile = df.groupby('userId')\\\n",
    "        .agg( \n",
    "\n",
    "            # first(col('state')).alias('state'),\n",
    "            first(when(col('gender') == 'M', TRUE).otherwise(FALSE)).alias('male'),\n",
    "\n",
    "            smin(col('first_ts')).alias('ts_start'),\n",
    "            smax(col('last_event_ts')).alias('ts_end'),        \n",
    "        \n",
    "            ((smax(col('last_event_ts')) - smin(col('first_ts'))) / milliseconds_to_hours).alias('time_window'),\n",
    "        \n",
    "            # Subscription\n",
    "            count(when(col('page') == 'Submit Downgrade', True)).alias('n_downgrades'),\n",
    "            count(when(col('page') == 'Submit Upgrade', True)).alias('n_upgrades'),\n",
    "            last(when(col('level') == 'paid', TRUE).otherwise(FALSE)).alias('paid'),\n",
    "            first(when(col('last_page') == CHURN_CANCELLATION_PAGE, TRUE).otherwise(FALSE)).alias('canceled'),\n",
    "\n",
    "            # Streaming\n",
    "            count(when(col('page') == 'NextSong', True)).alias('n_songs'),\n",
    "            count(when(col('page') == 'Thumbs Up', True)).alias('n_likes'),\n",
    "            count(when(col('page') == 'Thumbs Down', True)).alias('n_dislikes'),\n",
    "            countDistinct(col('sessionId')).alias('n_sess'),\n",
    "            (avg(col('session_duration')) / milliseconds_to_hours).alias('avg_session_duration'),\n",
    "\n",
    "            # Community\n",
    "            count(when(col('page') == 'Add Friend', True)).alias('n_friends'),\n",
    "            count(when(col('page') == 'Add to Playlist', True)).alias('n_added_to_playlist'),\n",
    "\n",
    "            # Other\n",
    "            count(when(col('page') == 'Home', True)).alias('n_home'),\n",
    "            count(when(col('page') == 'Roll Advert', True)).alias('n_ads'),\n",
    "            count(when(col('page') == 'Help', True)).alias('n_help'),\n",
    "            count(when(col('page') == 'Error', True)).alias('n_errors'),\n",
    "            count(when(col('page') == 'Settings', True)).alias('n_settings'),\n",
    "            count(col('page')).alias('n_actions')\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Location\n",
    "    # states = list(map(lambda c: c[0].strip(), df.select(['state']).distinct().rdd.collect()))\n",
    "    # for state in states:\n",
    "    #    df_user_profile = df_user_profile.withColumn(state.lower(), when(df_user_profile.state == state, 1).otherwise(0))\n",
    "    \n",
    "    return df_user_profile\n",
    "\n",
    "def create_days_dimension(df):\n",
    "    \n",
    "    df_unique_days = df.groupby('userId').agg(countDistinct('date').alias('n_days'))\n",
    "    \n",
    "    df_daily_actions = df.groupby('userId', 'date').agg(count('page').alias('total'))\n",
    "    df_daily_actions = df_daily_actions.groupby('userId').agg(avg('total').alias('avg_daily_actions')) \n",
    "\n",
    "    df_days = df_unique_days.join(df_daily_actions, df_unique_days.userId == df_daily_actions.userId)\n",
    "    \n",
    "    # Remove duplicated column after join\n",
    "    df_days = df_days.drop(df_daily_actions.userId)\n",
    "    \n",
    "    return df_days\n",
    "\n",
    "def sort_features(df, columns_order):\n",
    "    _columns = df.columns\n",
    "    _columns.sort()\n",
    "    \n",
    "    for _idx, _val in list(enumerate(columns_order)):\n",
    "        _columns.pop(_columns.index(_val))\n",
    "        _columns.insert(_idx, _val)\n",
    "        \n",
    "    assert len(_columns) == len(df.columns)\n",
    "\n",
    "    return _columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:17.409493Z",
     "start_time": "2020-05-03T18:54:17.395923Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_features = [ 'paid', 'male' ]\n",
    "\n",
    "numeric_features = [\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_all = [\n",
    "    'canceled',\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]\n",
    "\n",
    "columns_to_train = [\n",
    "    'male',\n",
    "    'paid',\n",
    "    'avg_daily_actions',\n",
    "    'avg_session_duration', \n",
    "    'n_actions',\n",
    "    'n_added_to_playlist',\n",
    "    'n_ads',\n",
    "    'n_days',\n",
    "    'n_dislikes',\n",
    "    'n_downgrades',\n",
    "    'n_errors',\n",
    "    'n_friends',\n",
    "    'n_help',\n",
    "    'n_home',\n",
    "    'n_likes',\n",
    "    'n_sess',\n",
    "    'n_settings',\n",
    "    'n_songs',\n",
    "    'n_upgrades', \n",
    "    'session_avg_actions',\n",
    "    'session_avg_added_playlist',\n",
    "    'session_avg_ads',\n",
    "    'session_avg_dislikes',\n",
    "    'session_avg_errors',\n",
    "    'session_avg_friends',\n",
    "    'session_avg_help',\n",
    "    'session_avg_home',\n",
    "    'session_avg_likes',\n",
    "    'session_avg_playback',\n",
    "    'session_avg_settings',\n",
    "    'session_avg_songs',\n",
    "    'session_avg_time_away',\n",
    "    'time_window'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data - create a unique row per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:56.336626Z",
     "start_time": "2020-05-03T18:54:55.625606Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sessions = create_session_dimension(df)\n",
    "df_days = create_days_dimension(df)\n",
    "\n",
    "df_users = create_user_dimension(df)\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_days, on = 'userId')\n",
    "\n",
    "_columns = sort_features(df_users, [ 'userId', 'male', 'paid', 'canceled'])\n",
    "_columns = list(set(df_users.schema.names + df_sessions.schema.names) - set(['ts_start', 'ts_end', 'state']))\n",
    "\n",
    "df_users = df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:58.923146Z",
     "start_time": "2020-05-03T18:54:58.785312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "df_users = df_users.select(columns_all).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:55:02.193582Z",
     "start_time": "2020-05-03T18:55:02.181442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- canceled: integer (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- avg_daily_actions: double (nullable = false)\n",
      " |-- avg_session_duration: double (nullable = false)\n",
      " |-- n_actions: long (nullable = false)\n",
      " |-- n_added_to_playlist: long (nullable = false)\n",
      " |-- n_ads: long (nullable = false)\n",
      " |-- n_days: long (nullable = false)\n",
      " |-- n_dislikes: long (nullable = false)\n",
      " |-- n_downgrades: long (nullable = false)\n",
      " |-- n_errors: long (nullable = false)\n",
      " |-- n_friends: long (nullable = false)\n",
      " |-- n_help: long (nullable = false)\n",
      " |-- n_home: long (nullable = false)\n",
      " |-- n_likes: long (nullable = false)\n",
      " |-- n_sess: long (nullable = false)\n",
      " |-- n_settings: long (nullable = false)\n",
      " |-- n_songs: long (nullable = false)\n",
      " |-- n_upgrades: long (nullable = false)\n",
      " |-- session_avg_actions: double (nullable = false)\n",
      " |-- session_avg_added_playlist: double (nullable = false)\n",
      " |-- session_avg_ads: double (nullable = false)\n",
      " |-- session_avg_dislikes: double (nullable = false)\n",
      " |-- session_avg_errors: double (nullable = false)\n",
      " |-- session_avg_friends: double (nullable = false)\n",
      " |-- session_avg_help: double (nullable = false)\n",
      " |-- session_avg_home: double (nullable = false)\n",
      " |-- session_avg_likes: double (nullable = false)\n",
      " |-- session_avg_playback: double (nullable = false)\n",
      " |-- session_avg_settings: double (nullable = false)\n",
      " |-- session_avg_songs: double (nullable = false)\n",
      " |-- session_avg_time_away: double (nullable = false)\n",
      " |-- time_window: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:56:02.984420Z",
     "start_time": "2020-05-03T18:56:02.339239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:07.907622Z",
     "start_time": "2020-05-03T18:56:06.057731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------\n",
      " canceled                   | 0                   \n",
      " male                       | 1                   \n",
      " paid                       | 1                   \n",
      " avg_daily_actions          | 56.833333333333336  \n",
      " avg_session_duration       | 8.064825676115998   \n",
      " n_actions                  | 682                 \n",
      " n_added_to_playlist        | 20                  \n",
      " n_ads                      | 3                   \n",
      " n_days                     | 12                  \n",
      " n_dislikes                 | 10                  \n",
      " n_downgrades               | 1                   \n",
      " n_errors                   | 3                   \n",
      " n_friends                  | 12                  \n",
      " n_help                     | 5                   \n",
      " n_home                     | 27                  \n",
      " n_likes                    | 25                  \n",
      " n_sess                     | 9                   \n",
      " n_settings                 | 4                   \n",
      " n_songs                    | 557                 \n",
      " n_upgrades                 | 0                   \n",
      " session_avg_actions        | 17.669538182524775  \n",
      " session_avg_added_playlist | 0.5181682751473542  \n",
      " session_avg_ads            | 0.07772524127210312 \n",
      " session_avg_dislikes       | 0.2590841375736771  \n",
      " session_avg_errors         | 0.07772524127210312 \n",
      " session_avg_friends        | 0.3109009650884125  \n",
      " session_avg_help           | 0.12954206878683855 \n",
      " session_avg_home           | 0.6995271714489281  \n",
      " session_avg_likes          | 0.6477103439341926  \n",
      " session_avg_playback       | 1.0167933265683589  \n",
      " session_avg_settings       | 0.10363365502947082 \n",
      " session_avg_songs          | 14.430986462853811  \n",
      " session_avg_time_away      | 152.10534722222224  \n",
      " time_window                | 1255.4402777777777  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.show(1, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:36.229022Z",
     "start_time": "2020-05-03T18:57:36.216069Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "import sys\n",
    "\n",
    "w = Window().partitionBy()\n",
    "\n",
    "def z_score(col, w):\n",
    "    _avg_ = avg(col).over(w)\n",
    "    avg_sq = avg(spow(col, 2)).over(w)\n",
    "    sd_ = sqrt(avg_sq - spow(_avg_, 2))\n",
    "    return sabs((col - _avg_) / sd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:37.763838Z",
     "start_time": "2020-05-03T18:57:37.759089Z"
    }
   },
   "outputs": [],
   "source": [
    " _columns_to_check_outliers = [ 'avg_daily_actions', 'avg_session_duration',  'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:39.256145Z",
     "start_time": "2020-05-03T18:57:38.862852Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in _columns_to_check_outliers:\n",
    "    df_users = df_users.withColumn(f'zscore_{c}', z_score(col(c), w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:40.314767Z",
     "start_time": "2020-05-03T18:57:40.310779Z"
    }
   },
   "outputs": [],
   "source": [
    "zscore_columns = []\n",
    "\n",
    "for c in _columns_to_check_outliers:\n",
    "    zscore_columns.append(f'zscore_{c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:41.251562Z",
     "start_time": "2020-05-03T18:57:41.246805Z"
    }
   },
   "outputs": [],
   "source": [
    "_query = ''\n",
    "_threshold = 3\n",
    "\n",
    "for c in zscore_columns:\n",
    "    _begin = ' and ' if len(_query) > 0 else ''\n",
    "    _query += f'{_begin}{c} < {_threshold}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:41:52.215097Z",
     "start_time": "2020-05-02T23:41:52.210639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zscore_avg_daily_actions < 3 and zscore_avg_session_duration < 3 and zscore_session_avg_actions < 3 and zscore_session_avg_added_playlist < 3 and zscore_session_avg_ads < 3 and zscore_session_avg_dislikes < 3 and zscore_session_avg_errors < 3 and zscore_session_avg_friends < 3 and zscore_session_avg_help < 3 and zscore_session_avg_home < 3 and zscore_session_avg_likes < 3 and zscore_session_avg_playback < 3 and zscore_session_avg_settings < 3 and zscore_session_avg_songs < 3 and zscore_session_avg_time_away < 3 and zscore_time_window < 3'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:42:18.043231Z",
     "start_time": "2020-05-02T23:42:16.788785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22278"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:48.300743Z",
     "start_time": "2020-05-03T18:57:48.096237Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users = df_users.filter(_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:57:58.258784Z",
     "start_time": "2020-05-03T18:57:52.875913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------\n",
      " canceled                          | 0                    \n",
      " male                              | 1                    \n",
      " paid                              | 1                    \n",
      " avg_daily_actions                 | 56.833333333333336   \n",
      " avg_session_duration              | 8.064825676115998    \n",
      " n_actions                         | 682                  \n",
      " n_added_to_playlist               | 20                   \n",
      " n_ads                             | 3                    \n",
      " n_days                            | 12                   \n",
      " n_dislikes                        | 10                   \n",
      " n_downgrades                      | 1                    \n",
      " n_errors                          | 3                    \n",
      " n_friends                         | 12                   \n",
      " n_help                            | 5                    \n",
      " n_home                            | 27                   \n",
      " n_likes                           | 25                   \n",
      " n_sess                            | 9                    \n",
      " n_settings                        | 4                    \n",
      " n_songs                           | 557                  \n",
      " n_upgrades                        | 0                    \n",
      " session_avg_actions               | 17.669538182524775   \n",
      " session_avg_added_playlist        | 0.5181682751473542   \n",
      " session_avg_ads                   | 0.07772524127210312  \n",
      " session_avg_dislikes              | 0.2590841375736771   \n",
      " session_avg_errors                | 0.07772524127210312  \n",
      " session_avg_friends               | 0.3109009650884125   \n",
      " session_avg_help                  | 0.12954206878683855  \n",
      " session_avg_home                  | 0.6995271714489281   \n",
      " session_avg_likes                 | 0.6477103439341926   \n",
      " session_avg_playback              | 1.0167933265683589   \n",
      " session_avg_settings              | 0.10363365502947082  \n",
      " session_avg_songs                 | 14.430986462853811   \n",
      " session_avg_time_away             | 152.10534722222224   \n",
      " time_window                       | 1255.4402777777777   \n",
      " zscore_avg_daily_actions          | 0.20603868603291917  \n",
      " zscore_avg_session_duration       | 0.11185436365031809  \n",
      " zscore_session_avg_actions        | 0.021949561538381478 \n",
      " zscore_session_avg_added_playlist | 0.5638574813568572   \n",
      " zscore_session_avg_ads            | 0.2513579882358607   \n",
      " zscore_session_avg_dislikes       | 0.2799020941837383   \n",
      " zscore_session_avg_errors         | 1.0300653017687413   \n",
      " zscore_session_avg_friends        | 0.014745929674361564 \n",
      " zscore_session_avg_help           | 0.2471005767549517   \n",
      " zscore_session_avg_home           | 0.014799397650090883 \n",
      " zscore_session_avg_likes          | 0.31841505510121293  \n",
      " zscore_session_avg_playback       | 0.018363431277507183 \n",
      " zscore_session_avg_settings       | 0.022953803926210874 \n",
      " zscore_session_avg_songs          | 0.014381514863177054 \n",
      " zscore_session_avg_time_away      | 0.09843974859677213  \n",
      " zscore_time_window                | 0.6008273431313189   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.show(1, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T22:36:41.965319Z",
     "start_time": "2020-05-02T22:36:41.456564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T22:37:47.305799Z",
     "start_time": "2020-05-02T22:37:44.234086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22278"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_filtered = df[(np.abs(stats.zscore(df[_columns_to_check_outliers])) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T18:00:04.233835Z",
     "start_time": "2020-05-02T18:00:03.769010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " n_dislikes                 | 5                    \n",
      " session_avg_settings       | 0.09123393902531743  \n",
      " n_help                     | 1                    \n",
      " userId                     | 200002               \n",
      " n_actions                  | 395                  \n",
      " session_avg_help           | 0.045616969512658714 \n",
      " n_settings                 | 2                    \n",
      " session_avg_playback       | 0.997184759370486    \n",
      " n_sess                     | 5                    \n",
      " avg_session_duration       | 5.75821870604782     \n",
      " session_avg_home           | 1.0491902987911503   \n",
      " n_downgrades               | 0                    \n",
      " canceled                   | 1                    \n",
      " session_avg_likes          | 0.6842545426898806   \n",
      " n_likes                    | 15                   \n",
      " session_avg_dislikes       | 0.22808484756329356  \n",
      " n_songs                    | 310                  \n",
      " n_upgrades                 | 1                    \n",
      " n_ads                      | 11                   \n",
      " n_days                     | 6                    \n",
      " time_window                | 689.8877777777777    \n",
      " n_added_to_playlist        | 6                    \n",
      " session_avg_friends        | 0.09123393902531743  \n",
      " n_friends                  | 2                    \n",
      " avg_daily_actions          | 65.83333333333333    \n",
      " paid                       | 0                    \n",
      " session_avg_time_away      | 166.99152777777778   \n",
      " session_avg_ads            | 0.5017866646392458   \n",
      " session_avg_errors         | 0.0                  \n",
      " session_avg_actions        | 18.018702957500192   \n",
      " n_home                     | 23                   \n",
      " session_avg_added_playlist | 0.27370181707595226  \n",
      " n_errors                   | 0                    \n",
      " male                       | 1                    \n",
      " session_avg_songs          | 14.1412605489242     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_users.where(df_users.userId == int(200002)).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T17:30:06.552478Z",
     "start_time": "2020-05-02T17:28:15.720923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " canceled                   | 0    \n",
      " male                       | 1    \n",
      " paid                       | 1    \n",
      " avg_daily_actions          | 57   \n",
      " avg_session_duration       | 8    \n",
      " n_actions                  | 682  \n",
      " n_added_to_playlist        | 20   \n",
      " n_ads                      | 3    \n",
      " n_days                     | 12   \n",
      " n_dislikes                 | 10   \n",
      " n_downgrades               | 1    \n",
      " n_errors                   | 3    \n",
      " n_friends                  | 12   \n",
      " n_help                     | 5    \n",
      " n_home                     | 27   \n",
      " n_likes                    | 25   \n",
      " n_sess                     | 9    \n",
      " n_settings                 | 4    \n",
      " n_songs                    | 557  \n",
      " n_upgrades                 | 0    \n",
      " session_avg_actions        | 18   \n",
      " session_avg_added_playlist | 1    \n",
      " session_avg_ads            | 0    \n",
      " session_avg_dislikes       | 0    \n",
      " session_avg_errors         | 0    \n",
      " session_avg_friends        | 0    \n",
      " session_avg_help           | 0    \n",
      " session_avg_home           | 1    \n",
      " session_avg_likes          | 1    \n",
      " session_avg_playback       | 1    \n",
      " session_avg_settings       | 0    \n",
      " session_avg_songs          | 14   \n",
      " session_avg_time_away      | 152  \n",
      " time_window                | 1255 \n",
      "-RECORD 1--------------------------\n",
      " canceled                   | 1    \n",
      " male                       | 0    \n",
      " paid                       | 1    \n",
      " avg_daily_actions          | 58   \n",
      " avg_session_duration       | 4    \n",
      " n_actions                  | 349  \n",
      " n_added_to_playlist        | 12   \n",
      " n_ads                      | 2    \n",
      " n_days                     | 6    \n",
      " n_dislikes                 | 2    \n",
      " n_downgrades               | 0    \n",
      " n_errors                   | 1    \n",
      " n_friends                  | 7    \n",
      " n_help                     | 1    \n",
      " n_home                     | 14   \n",
      " n_likes                    | 10   \n",
      " n_sess                     | 7    \n",
      " n_settings                 | 1    \n",
      " n_songs                    | 287  \n",
      " n_upgrades                 | 1    \n",
      " session_avg_actions        | 17   \n",
      " session_avg_added_playlist | 1    \n",
      " session_avg_ads            | 0    \n",
      " session_avg_dislikes       | 0    \n",
      " session_avg_errors         | 0    \n",
      " session_avg_friends        | 0    \n",
      " session_avg_help           | 0    \n",
      " session_avg_home           | 1    \n",
      " session_avg_likes          | 0    \n",
      " session_avg_playback       | 1    \n",
      " session_avg_settings       | 0    \n",
      " session_avg_songs          | 14   \n",
      " session_avg_time_away      | 65   \n",
      " time_window                | 410  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### WARN: Only round to display\n",
    "# Enforces the order for some columns\n",
    "df_users.select([sround(c, 0).cast(dataType = IntegerType()).alias(c) for c in columns_all]).show(2, True, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Save the final dataset to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T00:58:15.419170Z",
     "start_time": "2020-04-03T00:56:25.601997Z"
    }
   },
   "outputs": [],
   "source": [
    "df_users.select(columns_all).fillna(0).toPandas().to_csv('sparkify_data_full_dataset_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-21T20:49:53.790091Z",
     "start_time": "2020-03-21T20:49:53.293466Z"
    }
   },
   "outputs": [],
   "source": [
    "df.agg(countDistinct(df.userId).alias('unique_users')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.orderBy(df_users.userId).join(df_sessions, on = 'userId').select(_columns).groupBy('canceled').agg(count(df_users.canceled).alias('total')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advertises number (per session and all)\n",
    "    - The user **100010** returned after some idle time and received a considerable amount of advertises;\n",
    "    - Also, after thumbs down, I received two advertisements on four sounds. Then canceled the service.\n",
    "- Number of sessions\n",
    "- Paid subscription time\n",
    "- Avg songs before an ad\n",
    "- Number of skipped songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_date(df.ts.cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.userId == user_id).select(['artist',\n",
    " 'auth',\n",
    " 'firstName',\n",
    " 'gender',\n",
    " 'itemInSession',\n",
    " 'lastName',\n",
    " 'length',\n",
    " 'level', \n",
    " 'page',\n",
    " 'sessionId',\n",
    " 'song', \n",
    " 'ts', \n",
    " 'userId']).orderBy('sessionId', 'itemInSession').withColumn('datetime', date_format((df.ts/1000).cast(dataType=TimestampType()), 'HH:mm:ss dd-MM-YYYY')).show(350, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:45:23.455220Z",
     "start_time": "2020-05-02T23:45:23.449169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n",
      "\n",
      "Columns to train: ['avg_daily_actions', 'avg_session_duration', 'canceled', 'male', 'n_actions', 'n_added_to_playlist', 'n_ads', 'n_days', 'n_dislikes', 'n_downgrades', 'n_errors', 'n_friends', 'n_help', 'n_home', 'n_likes', 'n_sess', 'n_settings', 'n_songs', 'n_upgrades', 'paid', 'session_avg_actions', 'session_avg_added_playlist', 'session_avg_ads', 'session_avg_dislikes', 'session_avg_errors', 'session_avg_friends', 'session_avg_help', 'session_avg_home', 'session_avg_likes', 'session_avg_playback', 'session_avg_settings', 'session_avg_songs', 'session_avg_time_away', 'time_window']\n"
     ]
    }
   ],
   "source": [
    "columns_to_exclude = set(['userId'] + zscore_columns)\n",
    "\n",
    "columns_to_use = list(set(df_users.columns) - columns_to_exclude)\n",
    "\n",
    "columns_to_train = list(set(columns_to_use) - set(['canc']))\n",
    "\n",
    "columns_to_use.sort()\n",
    "columns_to_train.sort()\n",
    "\n",
    "print(f'Columns: {columns_to_use}\\n')\n",
    "print(f'Columns to train: {columns_to_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:40:23.879609Z",
     "start_time": "2020-05-03T21:40:23.875488Z"
    }
   },
   "outputs": [],
   "source": [
    "CHURN_LABEL = 'canceled'\n",
    "TRAIN_SPLIT_RATIO = .8\n",
    "TEST_SPLIT_RATIO = .2\n",
    "\n",
    "SPLIT_RATIO = [TRAIN_SPLIT_RATIO, TEST_SPLIT_RATIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:18:59.430597Z",
     "start_time": "2020-05-03T22:18:59.404356Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_predictions):\n",
    "    \n",
    "    # auc = roc_auc_score(y_test, y_predictions)\n",
    "    cm = confusion_matrix(y_test, y_predictions, labels = [1, 0])\n",
    "    \n",
    "    tn = cm[1, 1]\n",
    "    tp = cm[0, 0]\n",
    "    fp = cm[1, 0]\n",
    "    fn = cm[0, 1]\n",
    "    \n",
    "    total = np.sum(cm) # tn + tp + fn + fp\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = (tp) / (tp + fp)\n",
    "    recall = (tp) / (tp + fn) \n",
    "    \n",
    "    print(cm)\n",
    "\n",
    "def evaluate_multiclass_classifier(predictions, columns):\n",
    "    metrics_to_evaluate = [ 'accuracy', 'f1', 'weightedPrecision', 'weightedRecall', 'recall']\n",
    "    \n",
    "    result = {}\n",
    "    for metric in metrics_to_evaluate:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol = columns[0], predictionCol = columns[1], metricName = metric)\n",
    "        value = evaluator.evaluate(predictions)\n",
    "        result[metric] = value\n",
    "        print(f'{metric}: {value}') \n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_random_forest_classifier(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    # f_scaler = MaxAbsScaler(inputCol=\"num_features\", outputCol=\"num_features_escaled\")\n",
    "    # f_scaler = Normalizer(inputCol = \"num_features\", outputCol = \"num_features_escaled\", p = 3)\n",
    "    # f_scaler = MinMaxScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', )\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    # rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', numTrees = 10, maxBins = 5, impurity = 'entropy', minInstancesPerNode = 3, seed = 42)\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'idx_labels', featuresCol = 'features', seed = 42)\n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, rf_classifier, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def train_logistic_regression(data, columns, train_cloumns):\n",
    "    \n",
    "    # Split train/test\n",
    "    (train_df, test_df) = data.randomSplit(SPLIT_RATIO, seed = 42)\n",
    "    \n",
    "    # Create the indexer for labels\n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    \n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    \n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    \n",
    "    l_translator = IndexToString(inputCol = 'prediction', outputCol = 'predictedLabel', labels = [ 'Not churn', 'Churn' ])\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol = 'features', labelCol = 'idx_labels', maxIter = 1000, regParam = 0, elasticNetParam = 0, family = 'binomial', aggregationDepth = 15) \n",
    "    \n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, lr, l_translator ])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_df)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    return model.stages[2], predictions\n",
    "    \n",
    "\n",
    "def create_pipeline(model):\n",
    "    \n",
    "    l_indexer = StringIndexer(inputCol = CHURN_LABEL, outputCol = 'idx_labels')\n",
    "    f_binaries = VectorAssembler(inputCols = binary_features, outputCol = 'bin_features')\n",
    "    f_numeric = VectorAssembler(inputCols = numeric_features, outputCol = 'num_features')\n",
    "    f_scaler = StandardScaler(inputCol = 'num_features', outputCol = 'num_features_escaled', withStd = True, withMean = True)\n",
    "    f_all = VectorAssembler(inputCols = [ 'bin_features' , 'num_features_escaled' ], outputCol = 'features')\n",
    "    pipeline = Pipeline(stages = [ l_indexer, f_binaries, f_numeric, f_scaler, f_all, model ])\n",
    "    return pipeline\n",
    "\n",
    "def create_random_forest_pipeline():\n",
    "    rf_classifier = RandomForestClassifier(labelCol = 'canceled', featuresCol = 'features', seed = 42)\n",
    "    return create_pipeline(rf_classifier)\n",
    "\n",
    "def create_gradient_boost_pipeline():\n",
    "    gbt_classifier = GBTClassifier(labelCol = 'canceled', maxDepth = 5, maxIter = 100, seed = 42)\n",
    "    return create_pipeline(gbt_classifier)\n",
    "\n",
    "def create_logistic_regression_pipeline():\n",
    "    lr_classifier = LogisticRegression(featuresCol = 'features', labelCol = 'idx_labels', weightCol = 'class_weights') \n",
    "    return create_pipeline(lr_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:47.811584Z",
     "start_time": "2020-05-03T22:06:32.217201Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_logistic_regression(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:48.039665Z",
     "start_time": "2020-05-03T22:06:47.814507Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:06:48.049295Z",
     "start_time": "2020-05-03T22:06:48.041153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 537  394]\n",
      " [ 179 3036]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:39:28.738718Z",
     "start_time": "2020-05-03T19:39:28.722464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 912  690]\n",
      " [ 313 5305]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:33:43.529567Z",
     "start_time": "2020-05-03T19:33:43.514550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 875  727]\n",
      " [ 345 5273]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:03.408203Z",
     "start_time": "2020-05-03T19:22:52.520852Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:04.899188Z",
     "start_time": "2020-05-03T19:23:03.409659Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:23:06.416171Z",
     "start_time": "2020-05-03T19:23:06.400371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 824  778]\n",
      " [ 116 5502]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:16:46.273162Z",
     "start_time": "2020-05-03T19:16:46.260335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 737  666]\n",
      " [ 123 4712]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:09:12.152496Z",
     "start_time": "2020-05-03T19:09:12.137997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 734  669]\n",
      " [ 133 4702]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T23:32:41.611423Z",
     "start_time": "2020-04-04T23:32:31.035584Z"
    }
   },
   "outputs": [],
   "source": [
    "model, predictions = train_random_forest_classifier(df_users, columns_all, columns_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:46:31.224705Z",
     "start_time": "2020-05-02T23:46:31.215982Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T23:47:12.105081Z",
     "start_time": "2020-05-02T23:47:11.431773Z"
    }
   },
   "outputs": [],
   "source": [
    "df_r = predictions.select('canceled', 'prediction').toPandas()\n",
    "df_r['prediction'] = df_r.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:27:24.291425Z",
     "start_time": "2020-05-03T20:27:24.279015Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "\n",
    "class Recall(Evaluator):\n",
    "    \n",
    "    def __init__(self, predictionCol = 'prediction', labelCol = 'label'):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.uid = str(uuid.uuid4())\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        \n",
    "        tp = dataset.where((dataset[self.labelCol] == 1) & (dataset[self.predictionCol] == 1)).count()\n",
    "        fp = dataset.where((dataset[self.labelCol] == 0) & (dataset[self.predictionCol] == 1)).count()\n",
    "        tn = dataset.where((dataset[self.labelCol] == 0) & (dataset[self.predictionCol] == 0)).count()\n",
    "        fn = dataset.where((dataset[self.labelCol] == 1) & (dataset[self.predictionCol] == 0)).count()\n",
    "        \n",
    "        # fnr = fn / (1 if (tp + fn) == 0 else (tp + fn))\n",
    "        \n",
    "        return (100 / (tp + fn )) * tp\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:19:45.844236Z",
     "start_time": "2020-05-03T22:19:45.824404Z"
    }
   },
   "outputs": [],
   "source": [
    "_eval = BinaryClassificationEvaluator(rawPredictionCol = 'prediction', labelCol = 'canceled', metricName = 'areaUnderROC') # Recall(labelCol = 'canceled')\n",
    "\n",
    "def create_grid_search(pipeline, param_grid):\n",
    "    \n",
    "    return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = _eval, numFolds = 5, parallelism = 16, seed = 42)\n",
    "    # return CrossValidator(estimator = pipeline, estimatorParamMaps = param_grid, evaluator = _eval, numFolds = 3, parallelism = 16, seed = 42)\n",
    "\n",
    "def random_forest_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_rf = ParamGridBuilder().addGrid(model.maxDepth, [5, 10, 15, 20, 25]) \n",
    "    grid_rf = grid_rf.addGrid(model.impurity, ['gini']) \n",
    "    grid_rf = grid_rf.addGrid(model.maxBins, [5, 10, 15, 20, 25, 30, 35, 40])\n",
    "    grid_rf = grid_rf.addGrid(model.numTrees, [10, 20, 40, 60, 70])\n",
    "    grid_rf = grid_rf.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_rf)}')\n",
    "        \n",
    "    return create_grid_search(pipeline, grid_rf)\n",
    "\n",
    "def gradient_boost_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_gbt = ParamGridBuilder().addGrid(model.maxDepth, [2, 4, 6, 8, 10])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxIter, [20, 25, 40, 50, 100])\n",
    "    grid_gbt = grid_gbt.addGrid(model.maxBins, [2])\n",
    "    grid_gbt = grid_gbt.addGrid(model.subsamplingRate, [.5, .8, 1])\n",
    "    grid_gbt = grid_gbt.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_gbt)}')\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_gbt)\n",
    "\n",
    "def logistic_regression_grid_search(pipeline):\n",
    "    \n",
    "    model = pipeline.getStages()[-1]\n",
    "\n",
    "    grid_lr = ParamGridBuilder().addGrid(model.aggregationDepth, [2, 5, 10])\n",
    "    grid_lr = grid_lr.addGrid(model.elasticNetParam, [.0, .5, 1.0])\n",
    "    grid_lr = grid_lr.addGrid(model.fitIntercept, [ True, False ])\n",
    "    grid_lr = grid_lr.addGrid(model.standardization, [ True, False ])\n",
    "    grid_lr = grid_lr.addGrid(model.maxIter, [10, 100, 1000])\n",
    "    grid_lr = grid_lr.addGrid(model.regParam, [.0, .01, .5, 2.0])\n",
    "    grid_lr = grid_lr.addGrid(model.tol, [.0001])\n",
    "    grid_lr = grid_lr.addGrid(model.weightCol, [ 'class_weights' ])\n",
    "\n",
    "    grid_lr = grid_lr.build()\n",
    "    \n",
    "    print(f'Number of models to train: {len(grid_lr)}')\n",
    "   \n",
    "    return create_grid_search(pipeline, grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:19:49.000612Z",
     "start_time": "2020-05-03T22:19:48.995459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.2]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:41:51.541084Z",
     "start_time": "2020-05-03T21:41:51.524623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the new dataframe\n",
    "# data = df_users.select(columns_to_use).fillna(0)\n",
    "\n",
    "# Split train/test\n",
    "(train_df, test_df) = df_users.randomSplit(SPLIT_RATIO, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:41:54.226480Z",
     "start_time": "2020-05-03T21:41:53.748022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[canceled: int, male: int, paid: int, avg_daily_actions: double, avg_session_duration: double, n_actions: bigint, n_added_to_playlist: bigint, n_ads: bigint, n_days: bigint, n_dislikes: bigint, n_downgrades: bigint, n_errors: bigint, n_friends: bigint, n_help: bigint, n_home: bigint, n_likes: bigint, n_sess: bigint, n_settings: bigint, n_songs: bigint, n_upgrades: bigint, session_avg_actions: double, session_avg_added_playlist: double, session_avg_ads: double, session_avg_dislikes: double, session_avg_errors: double, session_avg_friends: double, session_avg_help: double, session_avg_home: double, session_avg_likes: double, session_avg_playback: double, session_avg_settings: double, session_avg_songs: double, session_avg_time_away: double, time_window: double, zscore_avg_daily_actions: double, zscore_avg_session_duration: double, zscore_session_avg_actions: double, zscore_session_avg_added_playlist: double, zscore_session_avg_ads: double, zscore_session_avg_dislikes: double, zscore_session_avg_errors: double, zscore_session_avg_friends: double, zscore_session_avg_help: double, zscore_session_avg_home: double, zscore_session_avg_likes: double, zscore_session_avg_playback: double, zscore_session_avg_settings: double, zscore_session_avg_songs: double, zscore_session_avg_time_away: double, zscore_time_window: double]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:41:58.393635Z",
     "start_time": "2020-05-03T21:41:56.011220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20875"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count() + test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:17:26.412542Z",
     "start_time": "2020-05-03T22:17:26.101740Z"
    }
   },
   "outputs": [],
   "source": [
    "balancing_ratio = train_df.filter(train_df.canceled == 0).count() / train_df.count()\n",
    "\n",
    "train_df = train_df.withColumn('class_weights', when(train_df.canceled == 1, balancing_ratio).otherwise(1 - balancing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:19:59.815556Z",
     "start_time": "2020-05-03T22:19:59.651085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------\n",
      " canceled                          | 0                   \n",
      " male                              | 0                   \n",
      " paid                              | 0                   \n",
      " avg_daily_actions                 | 1.0                 \n",
      " avg_session_duration              | 0.0                 \n",
      " n_actions                         | 1                   \n",
      " n_added_to_playlist               | 0                   \n",
      " n_ads                             | 0                   \n",
      " n_days                            | 1                   \n",
      " n_dislikes                        | 0                   \n",
      " n_downgrades                      | 0                   \n",
      " n_errors                          | 0                   \n",
      " n_friends                         | 0                   \n",
      " n_help                            | 0                   \n",
      " n_home                            | 0                   \n",
      " n_likes                           | 0                   \n",
      " n_sess                            | 1                   \n",
      " n_settings                        | 0                   \n",
      " n_songs                           | 1                   \n",
      " n_upgrades                        | 0                   \n",
      " session_avg_actions               | 0.0                 \n",
      " session_avg_added_playlist        | 0.0                 \n",
      " session_avg_ads                   | 0.0                 \n",
      " session_avg_dislikes              | 0.0                 \n",
      " session_avg_errors                | 0.0                 \n",
      " session_avg_friends               | 0.0                 \n",
      " session_avg_help                  | 0.0                 \n",
      " session_avg_home                  | 0.0                 \n",
      " session_avg_likes                 | 0.0                 \n",
      " session_avg_playback              | 0.0                 \n",
      " session_avg_settings              | 0.0                 \n",
      " session_avg_songs                 | 0.0                 \n",
      " session_avg_time_away             | 0.0                 \n",
      " time_window                       | 0.0                 \n",
      " zscore_avg_daily_actions          | 0.8139197798330571  \n",
      " zscore_avg_session_duration       | 1.4407264429725821  \n",
      " zscore_session_avg_actions        | 0.16660887754295364 \n",
      " zscore_session_avg_added_playlist | 2.030103907808543   \n",
      " zscore_session_avg_ads            | 0.28702009795893985 \n",
      " zscore_session_avg_dislikes       | 0.7413218756530036  \n",
      " zscore_session_avg_errors         | 0.3283143658389121  \n",
      " zscore_session_avg_friends        | 0.18681711287464223 \n",
      " zscore_session_avg_help           | 0.625071424372624   \n",
      " zscore_session_avg_home           | 0.03490681418982761 \n",
      " zscore_session_avg_likes          | 1.8344734597498906  \n",
      " zscore_session_avg_playback       | 0.35145585201324814 \n",
      " zscore_session_avg_settings       | 0.16854316310960377 \n",
      " zscore_session_avg_songs          | 0.3117850777239457  \n",
      " zscore_session_avg_time_away      | 0.9382256285773164  \n",
      " zscore_time_window                | 2.1691746279634074  \n",
      " class_weights                     | 0.22750911590650968 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:38:46.596580Z",
     "start_time": "2020-05-03T22:20:25.468519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 432\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_logistic_regression_pipeline()\n",
    "lr = logistic_regression_grid_search(pipeline)\n",
    "rf_results = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:43:40.106673Z",
     "start_time": "2020-05-03T20:34:07.085855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 216\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_logistic_regression_pipeline()\n",
    "lr = logistic_regression_grid_search(pipeline)\n",
    "rf_results = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:22.701524Z",
     "start_time": "2020-05-03T22:39:22.440311Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf_results.bestModel.transform(test_df)\n",
    "# predictions = rf_results.bestModel.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:25.013398Z",
     "start_time": "2020-05-03T22:39:24.789706Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:39:29.868076Z",
     "start_time": "2020-05-03T22:39:29.856256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 723  208]\n",
      " [ 443 2772]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:56:07.651259Z",
     "start_time": "2020-05-03T21:56:07.643173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 596  335]\n",
      " [ 340 2875]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:36:16.693411Z",
     "start_time": "2020-05-03T21:36:16.678291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 733  440]\n",
      " [ 423 3643]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:36:57.671472Z",
     "start_time": "2020-05-03T21:36:57.649482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2327  1237]\n",
      " [ 1268 10804]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:23:39.601559Z",
     "start_time": "2020-05-03T21:23:39.581082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048 1087]\n",
      " [1081 9439]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:19:17.856485Z",
     "start_time": "2020-05-03T21:19:17.839409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1016  586]\n",
      " [ 610 5008]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:44:25.403261Z",
     "start_time": "2020-05-03T20:44:25.395685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1016  586]\n",
      " [ 610 5008]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:41:06.460073Z",
     "start_time": "2020-05-03T22:41:06.454447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321030434677803"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rf_results.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T22:41:13.309427Z",
     "start_time": "2020-05-03T22:41:13.303652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5,\n",
       " {Param(parent='LogisticRegression_ef68b4aed01a', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 10,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='fitIntercept', doc='whether to fit an intercept term.'): False,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='maxIter', doc='max number of iterations (>= 0).'): 1000,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='regParam', doc='regularization parameter (>= 0).'): 2.0,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 0.0001,\n",
       "  Param(parent='LogisticRegression_ef68b4aed01a', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.'): 'class_weights'})"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(rf_results.avgMetrics, rf_results.getEstimatorParamMaps())))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T20:53:12.413935Z",
     "start_time": "2020-05-03T20:53:12.407383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_1f4e8dd33901', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='fitIntercept', doc='whether to fit an intercept term'): False,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='labelCol', doc='label column name'): 'idx_labels',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='maxIter', doc='maximum number of iterations (>= 0)'): 1000,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='regParam', doc='regularization parameter (>= 0)'): 0.0,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5,\n",
       " Param(parent='LogisticRegression_1f4e8dd33901', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T21:21:57.895954Z",
     "start_time": "2020-05-03T21:21:57.885690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7569201743883416,\n",
       " 0.7458524531641905,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.754917110340065,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.768159789539252,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7735699410985479,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7569201743883416,\n",
       " 0.7362262441393639,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7559669757940859,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7569201743883416,\n",
       " 0.734959860174886,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7533653046580704,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7569201743883416,\n",
       " 0.7458524531641905,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.754917110340065,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.768159789539252,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7735699410985479,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7569201743883416,\n",
       " 0.7362262441393639,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7559669757940859,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7569201743883416,\n",
       " 0.734959860174886,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7533653046580704,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7569201743883416,\n",
       " 0.7458524531641905,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7451379672521136,\n",
       " 0.5065533019305485,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.754917110340065,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.768159789539252,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7735699410985479,\n",
       " 0.7569633105158089,\n",
       " 0.6789049923471427,\n",
       " 0.6442362144668135,\n",
       " 0.7569201743883416,\n",
       " 0.7362262441393639,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7403064173269568,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7559669757940859,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7558486268810651,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7569201743883416,\n",
       " 0.734959860174886,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7656400367053511,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7686666415584142,\n",
       " 0.7353038442202462,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7598541367196125,\n",
       " 0.7533653046580704,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.768159789539252,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.7735699410985479,\n",
       " 0.7519631073496014,\n",
       " 0.5,\n",
       " 0.5]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:00:38.935148Z",
     "start_time": "2020-05-03T01:00:38.909068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 200\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_random_forest_pipeline()\n",
    "cv_rf = random_forest_grid_search(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:28:55.722489Z",
     "start_time": "2020-05-03T01:00:41.781911Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_rf_results = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:43.033732Z",
     "start_time": "2020-05-03T01:44:42.513443Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = cv_rf_results.bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:45.168583Z",
     "start_time": "2020-05-03T01:44:43.933933Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T01:44:54.443560Z",
     "start_time": "2020-05-03T01:44:54.431890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 605  326]\n",
      " [ 120 3095]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T14:20:10.954391Z",
     "start_time": "2020-04-05T14:20:10.948573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64.35829817256004,\n",
       " {Param(parent='RandomForestClassifier_5b6344e9b3f5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 25,\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
       "  Param(parent='RandomForestClassifier_5b6344e9b3f5', name='numTrees', doc='Number of trees to train (>= 1).'): 70})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(zip(cv_rf_results.avgMetrics, cv_rf_results.getEstimatorParamMaps())))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T02:50:44.206304Z",
     "start_time": "2020-05-03T01:45:20.673457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to train: 75\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T14:16:38.672206Z",
     "start_time": "2020-03-25T02:53:27.924737Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = create_gradient_boost_pipeline()\n",
    "cv_gbt = gradient_boost_grid_search(pipeline)\n",
    "_results = cv_gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T02:53:10.319833Z",
     "start_time": "2020-05-03T02:53:09.685054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 637  294]\n",
      " [ 274 2941]]\n"
     ]
    }
   ],
   "source": [
    "predictions = _results.bestModel.transform(test_df)\n",
    "\n",
    "df_results = predictions.select(['canceled', 'prediction']).toPandas()\n",
    "df_results['prediction'] = df_results.prediction.apply(int)\n",
    "\n",
    "plot_confusion_matrix(df_results['canceled'], df_results['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T02:01:07.081948Z",
     "start_time": "2020-03-23T02:01:07.074972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_ba2654d5e488', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): 'all',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='labelCol', doc='label column name'): 'canceled',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.'): 32,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxDepth', doc='Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxIter', doc='maximum number of iterations (>= 0)'): 20,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.'): 1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='seed', doc='random seed'): -6852653717611865011,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
       " Param(parent='GBTClassifier_ba2654d5e488', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimator params and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T14:06:56.517560Z",
     "start_time": "2020-03-01T14:06:56.488834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = cv_gbt_results.avgMetrics\n",
    "params = [{p.name: v for p, v in m.items()} for m in cv_gbt.getEstimatorParamMaps()]\n",
    "params_pd = pd.DataFrame(params)\n",
    "params_pd['score'] = scores\n",
    "params_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
